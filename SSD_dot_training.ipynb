{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSD_dot_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1vIY6pTc5Swe1NIPByPmslWatGH9hNDN5",
      "authorship_tag": "ABX9TyN1JqICWjgrfp8qApkhgGnM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoMurase/-/blob/master/SSD_dot_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWK4gZQ2yTNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/phoxter/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi_B30mwycVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path as osp\n",
        "import random\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k7wLYa5yfYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "from itertools import product as product\n",
        "from math import sqrt as sqrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mCCEm2Ayg7G",
        "colab_type": "code",
        "outputId": "3b2cd339-bb2e-4ec8-def7-016683b4f788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"使用デバイス：\", device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPXS5VPpykoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dot_df = pd.read_csv(path + 'train_dot.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42bMKX8sPLf9",
        "colab_type": "code",
        "outputId": "f15e3a69-4ee6-4423-ef30-86ded6d3cf03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "train_dot_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>box_name</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>13</td>\n",
              "      <td>101.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>8</td>\n",
              "      <td>176.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>10</td>\n",
              "      <td>249.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>309.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>6</td>\n",
              "      <td>321.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>387.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>0</td>\n",
              "      <td>398.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>464.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5707</th>\n",
              "      <td>dot/190906090318_805_001_B1011-27800-2.png</td>\n",
              "      <td>5</td>\n",
              "      <td>196.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5708</th>\n",
              "      <td>dot/190906090318_805_001_B1011-27800-2.png</td>\n",
              "      <td>10</td>\n",
              "      <td>273.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5709</th>\n",
              "      <td>dot/190906090318_805_001_B1011-27800-2.png</td>\n",
              "      <td>5</td>\n",
              "      <td>344.0</td>\n",
              "      <td>246.0</td>\n",
              "      <td>404.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5710</th>\n",
              "      <td>dot/190906090318_805_001_B1011-27800-2.png</td>\n",
              "      <td>0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>480.0</td>\n",
              "      <td>323.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5711</th>\n",
              "      <td>dot/190906090318_805_001_B1011-27800-2.png</td>\n",
              "      <td>2</td>\n",
              "      <td>494.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>554.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5712 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             img  box_name  ...   ymax  name\n",
              "0     dot/190809085119_805_001_B1011-27800-2.png        13  ...  277.0     H\n",
              "1     dot/190809085119_805_001_B1011-27800-2.png         8  ...  275.0     8\n",
              "2     dot/190809085119_805_001_B1011-27800-2.png        10  ...  237.0     -\n",
              "3     dot/190809085119_805_001_B1011-27800-2.png         6  ...  273.0     6\n",
              "4     dot/190809085119_805_001_B1011-27800-2.png         0  ...  275.0     0\n",
              "...                                          ...       ...  ...    ...   ...\n",
              "5707  dot/190906090318_805_001_B1011-27800-2.png         5  ...  326.0     5\n",
              "5708  dot/190906090318_805_001_B1011-27800-2.png        10  ...  288.0     -\n",
              "5709  dot/190906090318_805_001_B1011-27800-2.png         5  ...  326.0     5\n",
              "5710  dot/190906090318_805_001_B1011-27800-2.png         0  ...  323.0     0\n",
              "5711  dot/190906090318_805_001_B1011-27800-2.png         2  ...  330.0     2\n",
              "\n",
              "[5712 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3x7en9ZNI-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(path + '/labels.txt')\n",
        "lines = f.readlines()\n",
        "labels = {}\n",
        "count=0\n",
        "for line in lines:\n",
        "  line = line.replace('\\n','')\n",
        "  labels.update({count:line})\n",
        "  count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAhDuExhkPUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "3726d14d-df03-41c8-aa3e-c31a980b1de7"
      },
      "source": [
        "labels"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '0',\n",
              " 1: '1',\n",
              " 2: '2',\n",
              " 3: '3',\n",
              " 4: '4',\n",
              " 5: '5',\n",
              " 6: '6',\n",
              " 7: '7',\n",
              " 8: '8',\n",
              " 9: '9',\n",
              " 10: '-',\n",
              " 11: ':',\n",
              " 12: 'D',\n",
              " 13: 'H',\n",
              " 14: 'I',\n",
              " 15: 'K',\n",
              " 16: 'L',\n",
              " 17: 'M',\n",
              " 18: 'N',\n",
              " 19: 'O',\n",
              " 20: 'R',\n",
              " 21: 'S',\n",
              " 22: 'T',\n",
              " 23: 'U',\n",
              " 24: 'V',\n",
              " 25: 'B',\n",
              " 26: 'o'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBkO4MeN13X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = {'0','1','2','3','4','5',\n",
        " '6',\n",
        " '7',\n",
        "'8',\n",
        "'9',\n",
        " '-',\n",
        " ':',\n",
        " 'D',\n",
        " 'H',\n",
        " 'I',\n",
        " 'K',\n",
        " 'L',\n",
        " 'M',\n",
        " 'N',\n",
        " 'O',\n",
        " 'R',\n",
        " 'S',\n",
        " 'T',\n",
        " 'U',\n",
        " 'V',\n",
        " 'B',\n",
        " 'o'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On659n-dO9_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes_index = {}\n",
        "for i,c in enumerate(classes):\n",
        "  classes_index.update({c:i})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWlL8nKdRUlW",
        "colab_type": "code",
        "outputId": "c8888119-5b6c-422e-b5a7-d5ff7bcae779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "classes_index"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-': 3,\n",
              " '0': 17,\n",
              " '1': 25,\n",
              " '2': 26,\n",
              " '3': 4,\n",
              " '4': 13,\n",
              " '5': 1,\n",
              " '6': 10,\n",
              " '7': 7,\n",
              " '8': 11,\n",
              " '9': 15,\n",
              " ':': 8,\n",
              " 'B': 19,\n",
              " 'D': 16,\n",
              " 'H': 5,\n",
              " 'I': 2,\n",
              " 'K': 9,\n",
              " 'L': 12,\n",
              " 'M': 23,\n",
              " 'N': 0,\n",
              " 'O': 21,\n",
              " 'R': 14,\n",
              " 'S': 22,\n",
              " 'T': 18,\n",
              " 'U': 20,\n",
              " 'V': 24,\n",
              " 'o': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfLgtU0iPzfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dot_df['classes'] = train_dot_df['name'].replace(classes_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGYTD_uQRQqD",
        "colab_type": "code",
        "outputId": "16c90a4d-410a-4ce7-a77c-e279db4e1f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(train_dot_df['name'].value_counts() )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6fUPTAORiBC",
        "colab_type": "code",
        "outputId": "3850cc20-27be-4b18-f068-17f1c94bf8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len( train_dot_df.classes.value_counts() )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB390JmyYDIs",
        "colab_type": "code",
        "outputId": "f157730f-5c22-4979-e9f3-9b8e03709868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "classes_ = train_dot_df['classes'].unique()\n",
        "print(classes_)\n",
        "print(len(classes_) )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5 11  3 10 17  7 15 26  1  8 13 25  4  9 12 16  0 23 14 21 22 18 20 24\n",
            "  2]\n",
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbX7FMMpdfAT",
        "colab_type": "code",
        "outputId": "edd7afd3-7650-4407-b02c-f91b64748c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "train_dot_df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>box_name</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>name</th>\n",
              "      <th>classes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>13</td>\n",
              "      <td>101.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>H</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>8</td>\n",
              "      <td>176.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>10</td>\n",
              "      <td>249.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>309.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>-</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>6</td>\n",
              "      <td>321.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>387.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dot/190809085119_805_001_B1011-27800-2.png</td>\n",
              "      <td>0</td>\n",
              "      <td>398.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>464.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          img  box_name  ...  name  classes\n",
              "0  dot/190809085119_805_001_B1011-27800-2.png        13  ...     H        5\n",
              "1  dot/190809085119_805_001_B1011-27800-2.png         8  ...     8       11\n",
              "2  dot/190809085119_805_001_B1011-27800-2.png        10  ...     -        3\n",
              "3  dot/190809085119_805_001_B1011-27800-2.png         6  ...     6       10\n",
              "4  dot/190809085119_805_001_B1011-27800-2.png         0  ...     0       17\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAssuvB9bsTl",
        "colab_type": "code",
        "outputId": "65f81999-3f21-4bc9-9d70-7be8323c9453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "train_dot_df.info()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5712 entries, 0 to 5711\n",
            "Data columns (total 8 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   img       5712 non-null   object \n",
            " 1   box_name  5712 non-null   int64  \n",
            " 2   xmin      5712 non-null   float64\n",
            " 3   ymin      5712 non-null   float64\n",
            " 4   xmax      5712 non-null   float64\n",
            " 5   ymax      5712 non-null   float64\n",
            " 6   name      5712 non-null   object \n",
            " 7   classes   5712 non-null   int64  \n",
            "dtypes: float64(4), int64(2), object(2)\n",
            "memory usage: 357.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwS0P3QVyrCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ファイルパスのリストを取得\n",
        "#train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "#    rootpath)\n",
        "\n",
        "all_img_list = []\n",
        "\n",
        "for value in train_dot_df['img'].values:\n",
        "  all_img_list.append(path + value)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1PleXjfy0ep",
        "colab_type": "code",
        "outputId": "78ae81d8-5d97-4d55-a4e6-ef6b68b95b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "print(len(all_img_list))\n",
        "all_img_list[0:10]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/phoxter/dot/190809085119_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809085119_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809085119_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809085119_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809085119_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809085119_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809081818_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809081818_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809081818_805_001_B1011-27800-2.png',\n",
              " '/content/drive/My Drive/phoxter/dot/190809081818_805_001_B1011-27800-2.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q08Mdzgy9-k",
        "colab_type": "code",
        "outputId": "d8a4f089-f7e0-4670-bc1d-0f295aef183f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "TRAIN_SIZE = int (len(all_img_list) * 0.8 )\n",
        "\n",
        "train_img_list = all_img_list[0:TRAIN_SIZE]\n",
        "val_img_list = all_img_list[TRAIN_SIZE:]\n",
        "\n",
        "print(train_img_list[-1])\n",
        "print(val_img_list[0])\n",
        "print('train size:',len(train_img_list))\n",
        "print('validation size:',len(val_img_list))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/phoxter/dot/190903084934_805_001_B1011-27800-2.png\n",
            "/content/drive/My Drive/phoxter/dot/190903084934_805_001_B1011-27800-2.png\n",
            "train size: 4569\n",
            "validation size: 1143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-dIFPf5zAQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Anno_list():\n",
        "  def __call__(self, idx, df, width, height):\n",
        "    ret = []\n",
        "    ymin = df.at[idx,'ymin'] / height\n",
        "    xmin = df.at[idx,'xmin'] / width\n",
        "    ymax = df.at[idx,'ymax'] / height\n",
        "    xmax = df.at[idx,'xmax'] / width\n",
        "    #label = df.at[idx, 'box_name']\n",
        "    label = df.at[idx, 'box_name']\n",
        "    #label_idx = self.classes.index(name)\n",
        "    bndbox = [xmin, ymin, xmax, ymax, label]\n",
        "    ret += [bndbox]\n",
        "    return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZwT0qy2zNww",
        "colab_type": "code",
        "outputId": "be779868-7e10-4631-992c-4663b4a64fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"\n",
        "transform_anno = Anno_list()\n",
        "\n",
        "# 画像の読み込み OpenCVを使用\n",
        "ind = 1\n",
        "image_file_path = val_img_list[ind]\n",
        "img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "# アノテーションをリストで表示\n",
        "transform_anno(ind, train_dot_df, width, height)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntransform_anno = Anno_list()\\n\\n# 画像の読み込み OpenCVを使用\\nind = 1\\nimage_file_path = val_img_list[ind]\\nimg = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\\nheight, width, channels = img.shape  # 画像のサイズを取得\\n\\n# アノテーションをリストで表示\\ntransform_anno(ind, train_dot_df, width, height)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDZQ_cbszQpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# フォルダ「utils」にあるdata_augumentation.pyからimport。\n",
        "# 入力画像の前処理をするクラス\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/phoxter/utils')\n",
        "from data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
        "class DataTransform():\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                ConvertFromInts(),  # intをfloat32に変換\n",
        "               # ToAbsoluteCoords(),  # アノテーションデータの規格化を戻す\n",
        "               # PhotometricDistort(),  # 画像の色調などをランダムに変化\n",
        "               # Expand(color_mean),  # 画像のキャンバスを広げる\n",
        "               # RandomSampleCrop(),  # 画像内の部分をランダムに抜き出す\n",
        "               # RandomMirror(),  # 画像を反転させる\n",
        "               # ToPercentCoords(),  # アノテーションデータを0-1に規格化\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                ConvertFromInts(),  # intをfloatに変換\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "       \n",
        "        return self.data_transform[phase](img, boxes, labels) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKuC74R8zvva",
        "colab_type": "code",
        "outputId": "c47e72c4-60a4-49e2-c5d1-016063665e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# 動作の確認\n",
        "# 1. 画像読み込み\n",
        "idx = 0\n",
        "image_file_path = train_img_list[idx]\n",
        "img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "height, width, channels = img.shape  # 画像のサイズを取得\n",
        "print(height, width, channels)\n",
        "\n",
        "# アノテーションをリストで表示\n",
        "anno_list = transform_anno(idx, train_dot_df, width, height)\n",
        "\n",
        "# 3. 元画像の表示\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# 4. 前処理クラスの作成\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "transform = DataTransform(input_size, color_mean)\n",
        "\n",
        "# 5. train画像の表示\n",
        "phase = \"train\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 6. val画像の表示\n",
        "phase = \"val\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# 動作の確認\\n# 1. 画像読み込み\\nidx = 0\\nimage_file_path = train_img_list[idx]\\nimg = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\\nheight, width, channels = img.shape  # 画像のサイズを取得\\nprint(height, width, channels)\\n\\n# アノテーションをリストで表示\\nanno_list = transform_anno(idx, train_dot_df, width, height)\\n\\n# 3. 元画像の表示\\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\\nplt.show()\\n\\n# 4. 前処理クラスの作成\\ncolor_mean = (104, 117, 123)  # (BGR)の色の平均値\\ninput_size = 300  # 画像のinputサイズを300×300にする\\ntransform = DataTransform(input_size, color_mean)\\n\\n# 5. train画像の表示\\nphase = \"train\"\\nimg_transformed, boxes, labels = transform(\\n    img, phase, anno_list[:, :4], anno_list[:, 4])\\nplt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\\nplt.show()\\n\\n\\n# 6. val画像の表示\\nphase = \"val\"\\nimg_transformed, boxes, labels = transform(\\n    img, phase, anno_list[:, :4], anno_list[:, 4])\\nplt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\\nplt.show()\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byUJVUCvz26A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, img_list, anno_list,phase, transform, transform_anno):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase  # train もしくは valを指定\n",
        "        self.transform = transform  # 画像の変形\n",
        "        self.transform_anno = transform_anno  # アノテーションデータをxmlからリストへ\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        前処理をした画像のテンソル形式のデータとアノテーションを取得\n",
        "        '''\n",
        "        im, gt, h, w = self.pull_item(index)\n",
        "        return im, gt\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''前処理をした画像のテンソル形式のデータ、アノテーション、画像の高さ、幅を取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "        height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "        \"\"\"\n",
        "        # 2. xml形式のアノテーション情報をリストに\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
        "        \"\"\"\n",
        "        anno_list = self.transform_anno(index, train_dot_df, width, height)\n",
        "\n",
        "        # 3. 前処理を実施\n",
        "        img, boxes, labels = self.transform(\n",
        "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
        "\n",
        "        # 色チャネルの順番がBGRになっているので、RGBに順番変更\n",
        "        # さらに（高さ、幅、色チャネル）の順を（色チャネル、高さ、幅）に変換\n",
        "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "        # BBoxとラベルをセットにしたnp.arrayを作成、変数名「gt」はground truth（答え）の略称\n",
        "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4OGqSUpz-mH",
        "colab_type": "code",
        "outputId": "136d8ea1-f76b-4797-92bc-4ab95193c50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# 動作確認\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "\n",
        "#nameはラベル\n",
        "#anno_list = transform_anno(idx, train_stamp_df, width, height)\n",
        "#transform_anno(ind, train_stamp_df, width, height)\n",
        "train_dataset = Dataset(train_img_list,  phase=\"train\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_list())\n",
        "\n",
        "val_dataset = Dataset(val_img_list,  phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_list())\n",
        "\n",
        "\n",
        "# データの取り出し例\n",
        "val_dataset.__getitem__(1)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# 動作確認\\ncolor_mean = (104, 117, 123)  # (BGR)の色の平均値\\ninput_size = 300  # 画像のinputサイズを300×300にする\\n\\n#nameはラベル\\n#anno_list = transform_anno(idx, train_stamp_df, width, height)\\n#transform_anno(ind, train_stamp_df, width, height)\\ntrain_dataset = Dataset(train_img_list,  phase=\"train\", transform=DataTransform(\\n    input_size, color_mean), transform_anno=Anno_list())\\n\\nval_dataset = Dataset(val_img_list,  phase=\"val\", transform=DataTransform(\\n    input_size, color_mean), transform_anno=Anno_list())\\n\\n\\n# データの取り出し例\\nval_dataset.__getitem__(1)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeium5ux0Awx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def od_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Datasetから取り出すアノテーションデータのサイズが画像ごとに異なります。 \n",
        "    今回は画像の大きさは全て同じ。\n",
        "    画像内の物体数が2個であれば(2, 5)というサイズですが、3個であれば（3, 5）など変化します。\n",
        "    この変化に対応したDataLoaderを作成するために、\n",
        "    カスタイマイズした、collate_fnを作成します。\n",
        "    collate_fnは、PyTorchでリストからmini-batchを作成する関数です。\n",
        "    ミニバッチ分の画像が並んでいるリスト変数batchに、\n",
        "    ミニバッチ番号を指定する次元を先頭に1つ追加して、リストの形を変形します。\n",
        "    \"\"\"\n",
        "    \n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])  # sample[0] は画像imgです\n",
        "        targets.append(torch.FloatTensor(sample[1]))  # sample[1] はアノテーションgtです\n",
        "\n",
        "    # imgsはミニバッチサイズのリストになっています\n",
        "    # リストの要素はtorch.Size([3, 300, 300])です。\n",
        "    # このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換します\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # targetsはアノテーションデータの正解であるgtのリストです。\n",
        "    # リストのサイズはミニバッチサイズです。\n",
        "    # リストtargetsの要素は [n, 5] となっています。\n",
        "    # nは画像ごとに異なり、画像内にある物体の数となります。\n",
        "    # 5は [xmin, ymin, xmax, ymax, class_index] です\n",
        "\n",
        "    return imgs, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVZULxq-0ELp",
        "colab_type": "code",
        "outputId": "d2f6c967-4046-4767-dda4-ee8c621d91b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# データローダーの作成\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 動作の確認\n",
        "batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n",
        "images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n",
        "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
        "print(len(targets))\n",
        "print(targets[1].size())  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# データローダーの作成\\n\\nbatch_size = 4\\n\\ntrain_dataloader = data.DataLoader(\\n    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\\n\\nval_dataloader = data.DataLoader(\\n    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\\n\\n# 辞書型変数にまとめる\\ndataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\\n\\n# 動作の確認\\nbatch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\\nimages, targets = next(batch_iterator)  # 1番目の要素を取り出す\\nprint(images.size())  # torch.Size([4, 3, 300, 300])\\nprint(len(targets))\\nprint(targets[1].size())  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyACuvHj0HrO",
        "colab_type": "code",
        "outputId": "5b68735b-a7e6-40fa-8eaf-a3c24ef07108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "\"\"\"\n",
        "print(train_dataset.__len__())\n",
        "print(val_dataset.__len__())\n",
        "\"\"\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(train_dataset.__len__())\\nprint(val_dataset.__len__())\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoBwPp1B0JxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 34層にわたる、vggモジュールを作成\n",
        "def make_vgg():\n",
        "    layers = []\n",
        "    in_channels = 3  # 色チャネル数\n",
        "\n",
        "    # vggモジュールで使用する畳み込み層やマックスプーリングのチャネル数\n",
        "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256,\n",
        "           256, 'MC', 512, 512, 512, 'M', 512, 512, 512]\n",
        "\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif v == 'MC':\n",
        "            # ceilは出力サイズを、計算結果（float）に対して、切り上げで整数にするモード\n",
        "            # デフォルトでは出力サイズを計算結果（float）に対して、切り下げで整数にするfloorモード\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "\n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "    layers += [pool5, conv6,\n",
        "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
        "    return nn.ModuleList(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgIcCiMY0Msv",
        "colab_type": "code",
        "outputId": "dd83d70b-36ac-4944-ac63-c216c3dd85d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "# 動作確認\n",
        "vgg_test = make_vgg()\n",
        "print(vgg_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ModuleList(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "  (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
            "  (32): ReLU(inplace=True)\n",
            "  (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (34): ReLU(inplace=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD7iGCjA0Pof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 8層にわたる、extrasモジュールを作成\n",
        "def make_extras():\n",
        "    layers = []\n",
        "    in_channels = 1024  # vggモジュールから出力された、extraに入力される画像チャネル数\n",
        "\n",
        "    # extraモジュールの畳み込み層のチャネル数を設定するコンフィギュレーション\n",
        "    cfg = [256, 512, 128, 256, 128, 256, 128, 256]\n",
        "\n",
        "    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]\n",
        "    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]\n",
        "\n",
        "    return nn.ModuleList(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVVGDBjs_hpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# デフォルトボックスのオフセットを出力するloc_layers、\n",
        "# デフォルトボックスに対する各クラスの確率を出力するconf_layersを作成\n",
        "\n",
        "def make_loc_conf(num_classes=len(classes), bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
        "\n",
        "    loc_layers = []\n",
        "    conf_layers = []\n",
        "\n",
        "    # VGGの22層目、conv4_3（source1）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # VGGの最終層（source2）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source3）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source4）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source5）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source6）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eylz2fjr_hnJ",
        "colab_type": "code",
        "outputId": "338b57fe-f596-40e2-c5c9-57bff952d26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# 動作確認\n",
        "loc_test, conf_test = make_loc_conf()\n",
        "print(loc_test)\n",
        "print(conf_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ModuleList(\n",
            "  (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "ModuleList(\n",
            "  (0): Conv2d(512, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): Conv2d(1024, 162, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (2): Conv2d(512, 162, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): Conv2d(256, 162, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): Conv2d(256, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (5): Conv2d(256, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQvmbpuc_hjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convC4_3からの出力をscale=20のL2Normで正規化する層\n",
        "class L2Norm(nn.Module):\n",
        "    def __init__(self, input_channels=512, scale=20):\n",
        "        super(L2Norm, self).__init__()  # 親クラスのコンストラクタ実行\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_channels))\n",
        "        self.scale = scale  # 係数weightの初期値として設定する値\n",
        "        self.reset_parameters()  # パラメータの初期化\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        '''結合パラメータを大きさscaleの値にする初期化を実行'''\n",
        "        init.constant_(self.weight, self.scale)  # weightの値がすべてscale（=20）になる\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''38×38の特徴量に対して、512チャネルにわたって2乗和のルートを求めた\n",
        "        38×38個の値を使用し、各特徴量を正規化してから係数をかけ算する層'''\n",
        "\n",
        "        # 各チャネルにおける38×38個の特徴量のチャネル方向の2乗和を計算し、\n",
        "        # さらにルートを求め、割り算して正規化する\n",
        "        # normのテンソルサイズはtorch.Size([batch_num, 1, 38, 38])になります\n",
        "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n",
        "        x = torch.div(x, norm)\n",
        "\n",
        "        # 係数をかける。係数はチャネルごとに1つで、512個の係数を持つ\n",
        "        # self.weightのテンソルサイズはtorch.Size([512])なので\n",
        "        # torch.Size([batch_num, 512, 38, 38])まで変形します\n",
        "        weights = self.weight.unsqueeze(\n",
        "            0).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        out = weights * x\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Q1ILYu_xVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# デフォルトボックスを出力するクラス\n",
        "class DBox(object):\n",
        "    def __init__(self, cfg):\n",
        "        super(DBox, self).__init__()\n",
        "\n",
        "        # 初期設定\n",
        "        self.image_size = cfg['input_size']  # 画像サイズの300\n",
        "        # [38, 19, …] 各sourceの特徴量マップのサイズ\n",
        "        self.feature_maps = cfg['feature_maps']\n",
        "        self.num_priors = len(cfg[\"feature_maps\"])  # sourceの個数=6\n",
        "        self.steps = cfg['steps']  # [8, 16, …] DBoxのピクセルサイズ\n",
        "        self.min_sizes = cfg['min_sizes']  # [30, 60, …] 小さい正方形のDBoxのピクセルサイズ\n",
        "        self.max_sizes = cfg['max_sizes']  # [60, 111, …] 大きい正方形のDBoxのピクセルサイズ\n",
        "        self.aspect_ratios = cfg['aspect_ratios']  # 長方形のDBoxのアスペクト比\n",
        "\n",
        "    def make_dbox_list(self):\n",
        "        '''DBoxを作成する'''\n",
        "        mean = []\n",
        "        # 'feature_maps': [38, 19, 10, 5, 3, 1]\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            for i, j in product(range(f), repeat=2):  # fまでの数で2ペアの組み合わせを作る　f_P_2 個\n",
        "                # 特徴量の画像サイズ\n",
        "                # 300 / 'steps': [8, 16, 32, 64, 100, 300],\n",
        "                f_k = self.image_size / self.steps[k]\n",
        "\n",
        "                # DBoxの中心座標 x,y　ただし、0～1で規格化している\n",
        "                cx = (j + 0.5) / f_k\n",
        "                cy = (i + 0.5) / f_k\n",
        "\n",
        "                # アスペクト比1の小さいDBox [cx,cy, width, height]\n",
        "                # 'min_sizes': [30, 60, 111, 162, 213, 264]\n",
        "                s_k = self.min_sizes[k]/self.image_size\n",
        "                mean += [cx, cy, s_k, s_k]\n",
        "\n",
        "                # アスペクト比1の大きいDBox [cx,cy, width, height]\n",
        "                # 'max_sizes': [60, 111, 162, 213, 264, 315],\n",
        "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
        "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
        "\n",
        "                # その他のアスペクト比のdefBox [cx,cy, width, height]\n",
        "                for ar in self.aspect_ratios[k]:\n",
        "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
        "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
        "\n",
        "        # DBoxをテンソルに変換 torch.Size([8732, 4])\n",
        "        output = torch.Tensor(mean).view(-1, 4)\n",
        "\n",
        "        # DBoxの大きさが1を超えている場合は1にする\n",
        "        output.clamp_(max=1, min=0)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBnBR9G8_xRs",
        "colab_type": "code",
        "outputId": "2524923a-b566-4251-940e-ecbf74864ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "# 動作の確認\n",
        "\n",
        "# SSD300の設定\n",
        "ssd_cfg = {\n",
        "    'num_classes': len(classes) + 1,  # 背景クラスを含めた合計クラス数\n",
        "    'input_size': 300,  # 画像の入力サイズ\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "}\n",
        "\n",
        "# DBox作成\n",
        "dbox = DBox(ssd_cfg)\n",
        "dbox_list = dbox.make_dbox_list()\n",
        "\n",
        "# DBoxの出力を確認する\n",
        "pd.DataFrame(dbox_list.numpy())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.141421</td>\n",
              "      <td>0.141421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.141421</td>\n",
              "      <td>0.070711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.070711</td>\n",
              "      <td>0.141421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.013333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8727</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.502046</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8728</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.880000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8729</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.961249</td>\n",
              "      <td>0.961249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8730</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.622254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8731</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.622254</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8732 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2         3\n",
              "0     0.013333  0.013333  0.100000  0.100000\n",
              "1     0.013333  0.013333  0.141421  0.141421\n",
              "2     0.013333  0.013333  0.141421  0.070711\n",
              "3     0.013333  0.013333  0.070711  0.141421\n",
              "4     0.040000  0.013333  0.100000  0.100000\n",
              "...        ...       ...       ...       ...\n",
              "8727  0.833333  0.833333  0.502046  1.000000\n",
              "8728  0.500000  0.500000  0.880000  0.880000\n",
              "8729  0.500000  0.500000  0.961249  0.961249\n",
              "8730  0.500000  0.500000  1.000000  0.622254\n",
              "8731  0.500000  0.500000  0.622254  1.000000\n",
              "\n",
              "[8732 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbmom8fv_xP1",
        "colab_type": "code",
        "outputId": "07ae39d2-1c24-4bfd-972f-479268314a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# SSDクラスを作成する\n",
        "class SSD(nn.Module):\n",
        "\n",
        "    def __init__(self, phase, cfg):\n",
        "        super(SSD, self).__init__()\n",
        "\n",
        "        self.phase = phase  # train or inferenceを指定\n",
        "        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\n",
        "\n",
        "        # SSDのネットワークを作る\n",
        "        self.vgg = make_vgg()\n",
        "        self.extras = make_extras()\n",
        "        self.L2Norm = L2Norm()\n",
        "        self.loc, self.conf = make_loc_conf(\n",
        "            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n",
        "\n",
        "        # DBox作成\n",
        "        dbox = DBox(cfg)\n",
        "        self.dbox_list = dbox.make_dbox_list()\n",
        "\n",
        "        # 推論時はクラス「Detect」を用意します\n",
        "        if phase == 'inference':\n",
        "            self.detect = Detect()\n",
        "\n",
        "\n",
        "# 動作確認\n",
        "ssd_test = SSD(phase=\"train\", cfg=ssd_cfg)\n",
        "print(ssd_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SSD(\n",
            "  (vgg): ModuleList(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (34): ReLU(inplace=True)\n",
            "  )\n",
            "  (extras): ModuleList(\n",
            "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "  )\n",
            "  (L2Norm): L2Norm()\n",
            "  (loc): ModuleList(\n",
            "    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (conf): ModuleList(\n",
            "    (0): Conv2d(512, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(1024, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): Conv2d(512, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): Conv2d(256, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): Conv2d(256, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Conv2d(256, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7558s4I_xMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# オフセット情報を使い、DBoxをBBoxに変換する関数\n",
        "def decode(loc, dbox_list):\n",
        "    \"\"\"\n",
        "    オフセット情報を使い、DBoxをBBoxに変換する。\n",
        "    Parameters\n",
        "    ----------\n",
        "    loc:  [8732,4]\n",
        "        SSDモデルで推論するオフセット情報。\n",
        "    dbox_list: [8732,4]\n",
        "        DBoxの情報\n",
        "    Returns\n",
        "    -------\n",
        "    boxes : [xmin, ymin, xmax, ymax]\n",
        "        BBoxの情報\n",
        "    \"\"\"\n",
        "\n",
        "    # DBoxは[cx, cy, width, height]で格納されている\n",
        "    # locも[Δcx, Δcy, Δwidth, Δheight]で格納されている\n",
        "\n",
        "    # オフセット情報からBBoxを求める\n",
        "    boxes = torch.cat((\n",
        "        dbox_list[:, :2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\n",
        "        dbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2)), dim=1)\n",
        "    # boxesのサイズはtorch.Size([8732, 4])となります\n",
        "\n",
        "    # BBoxの座標情報を[cx, cy, width, height]から[xmin, ymin, xmax, ymax] に\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2  # 座標(xmin,ymin)へ変換\n",
        "    boxes[:, 2:] += boxes[:, :2]  # 座標(xmax,ymax)へ変換\n",
        "\n",
        "    return boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsz6BB0G__Gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Non-Maximum Suppressionを行う関数\n",
        "\n",
        "\n",
        "def nm_suppression(boxes, scores, overlap=0.45, top_k=200):\n",
        "    \"\"\"\n",
        "    Non-Maximum Suppressionを行う関数。\n",
        "    boxesのうち被り過ぎ（overlap以上）のBBoxを削除する。\n",
        "    Parameters\n",
        "    ----------\n",
        "    boxes : [確信度閾値（0.01）を超えたBBox数,4]\n",
        "        BBox情報。\n",
        "    scores :[確信度閾値（0.01）を超えたBBox数]\n",
        "        confの情報\n",
        "    Returns\n",
        "    -------\n",
        "    keep : リスト\n",
        "        confの降順にnmsを通過したindexが格納\n",
        "    count：int\n",
        "        nmsを通過したBBoxの数\n",
        "    \"\"\"\n",
        "\n",
        "    # returnのひな形を作成\n",
        "    count = 0\n",
        "    keep = scores.new(scores.size(0)).zero_().long()\n",
        "    # keep：torch.Size([確信度閾値を超えたBBox数])、要素は全部0\n",
        "\n",
        "    # 各BBoxの面積areaを計算\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    area = torch.mul(x2 - x1, y2 - y1)\n",
        "\n",
        "    # boxesをコピーする。後で、BBoxの被り度合いIOUの計算に使用する際のひな形として用意\n",
        "    tmp_x1 = boxes.new()\n",
        "    tmp_y1 = boxes.new()\n",
        "    tmp_x2 = boxes.new()\n",
        "    tmp_y2 = boxes.new()\n",
        "    tmp_w = boxes.new()\n",
        "    tmp_h = boxes.new()\n",
        "\n",
        "    # socreを昇順に並び変える\n",
        "    v, idx = scores.sort(0)\n",
        "\n",
        "    # 上位top_k個（200個）のBBoxのindexを取り出す（200個存在しない場合もある）\n",
        "    idx = idx[-top_k:]\n",
        "\n",
        "    # idxの要素数が0でない限りループする\n",
        "    while idx.numel() > 0:\n",
        "        i = idx[-1]  # 現在のconf最大のindexをiに\n",
        "\n",
        "        # keepの現在の最後にconf最大のindexを格納する\n",
        "        # このindexのBBoxと被りが大きいBBoxをこれから消去する\n",
        "        keep[count] = i\n",
        "        count += 1\n",
        "\n",
        "        # 最後のBBoxになった場合は、ループを抜ける\n",
        "        if idx.size(0) == 1:\n",
        "            break\n",
        "\n",
        "        # 現在のconf最大のindexをkeepに格納したので、idxをひとつ減らす\n",
        "        idx = idx[:-1]\n",
        "\n",
        "        # -------------------\n",
        "        # これからkeepに格納したBBoxと被りの大きいBBoxを抽出して除去する\n",
        "        # -------------------\n",
        "        # ひとつ減らしたidxまでのBBoxを、outに指定した変数として作成する\n",
        "        torch.index_select(x1, 0, idx, out=tmp_x1)\n",
        "        torch.index_select(y1, 0, idx, out=tmp_y1)\n",
        "        torch.index_select(x2, 0, idx, out=tmp_x2)\n",
        "        torch.index_select(y2, 0, idx, out=tmp_y2)\n",
        "\n",
        "        # すべてのBBoxに対して、現在のBBox=indexがiと被っている値までに設定(clamp)\n",
        "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i])\n",
        "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n",
        "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n",
        "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i])\n",
        "\n",
        "        # wとhのテンソルサイズをindexを1つ減らしたものにする\n",
        "        tmp_w.resize_as_(tmp_x2)\n",
        "        tmp_h.resize_as_(tmp_y2)\n",
        "\n",
        "        # clampした状態でのBBoxの幅と高さを求める\n",
        "        tmp_w = tmp_x2 - tmp_x1\n",
        "        tmp_h = tmp_y2 - tmp_y1\n",
        "\n",
        "        # 幅や高さが負になっているものは0にする\n",
        "        tmp_w = torch.clamp(tmp_w, min=0.0)\n",
        "        tmp_h = torch.clamp(tmp_h, min=0.0)\n",
        "\n",
        "        # clampされた状態での面積を求める\n",
        "        inter = tmp_w*tmp_h\n",
        "\n",
        "        # IoU = intersect部分 / (area(a) + area(b) - intersect部分)の計算\n",
        "        rem_areas = torch.index_select(area, 0, idx)  # 各BBoxの元の面積\n",
        "        union = (rem_areas - inter) + area[i]  # 2つのエリアのANDの面積\n",
        "        IoU = inter/union\n",
        "\n",
        "        # IoUがoverlapより小さいidxのみを残す\n",
        "        idx = idx[IoU.le(overlap)]  # leはLess than or Equal toの処理をする演算です\n",
        "        # IoUがoverlapより大きいidxは、最初に選んでkeepに格納したidxと同じ物体に対してBBoxを囲んでいるため消去\n",
        "\n",
        "    # whileのループが抜けたら終了\n",
        "\n",
        "    return keep, count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hab8Jnz_-_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SSDクラスを作成する\n",
        "class SSD(nn.Module):\n",
        "\n",
        "    def __init__(self, phase, cfg):\n",
        "        super(SSD, self).__init__()\n",
        "\n",
        "        self.phase = phase  # train or inferenceを指定\n",
        "        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\n",
        "\n",
        "        # SSDのネットワークを作る\n",
        "        self.vgg = make_vgg()\n",
        "        self.extras = make_extras()\n",
        "        self.L2Norm = L2Norm()\n",
        "        self.loc, self.conf = make_loc_conf(\n",
        "            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n",
        "\n",
        "        # DBox作成\n",
        "        dbox = DBox(cfg)\n",
        "        self.dbox_list = dbox.make_dbox_list()\n",
        "\n",
        "        # 推論時はクラス「Detect」を用意します\n",
        "        if phase == 'inference':\n",
        "            self.detect = Detect()\n",
        "\n",
        "    def forward(self, x):\n",
        "        sources = list()  # locとconfへの入力source1～6を格納\n",
        "        loc = list()  # locの出力を格納\n",
        "        conf = list()  # confの出力を格納\n",
        "\n",
        "        # vggのconv4_3まで計算する\n",
        "        for k in range(23):\n",
        "            x = self.vgg[k](x)\n",
        "\n",
        "        # conv4_3の出力をL2Normに入力し、source1を作成、sourcesに追加\n",
        "        source1 = self.L2Norm(x)\n",
        "        sources.append(source1)\n",
        "\n",
        "        # vggを最後まで計算し、source2を作成、sourcesに追加\n",
        "        for k in range(23, len(self.vgg)):\n",
        "            x = self.vgg[k](x)\n",
        "\n",
        "        sources.append(x)\n",
        "\n",
        "        # extrasのconvとReLUを計算\n",
        "        # source3～6を、sourcesに追加\n",
        "        for k, v in enumerate(self.extras):\n",
        "            x = F.relu(v(x), inplace=True)\n",
        "            if k % 2 == 1:  # conv→ReLU→cov→ReLUをしたらsourceに入れる\n",
        "                sources.append(x)\n",
        "\n",
        "        # source1～6に、それぞれ対応する畳み込みを1回ずつ適用する\n",
        "        # zipでforループの複数のリストの要素を取得\n",
        "        # source1～6まであるので、6回ループが回る\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
        "            # Permuteは要素の順番を入れ替え\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "            # l(x)とc(x)で畳み込みを実行\n",
        "            # l(x)とc(x)の出力サイズは[batch_num, 4*アスペクト比の種類数, featuremapの高さ, featuremap幅]\n",
        "            # sourceによって、アスペクト比の種類数が異なり、面倒なので順番入れ替えて整える\n",
        "            # permuteで要素の順番を入れ替え、\n",
        "            # [minibatch数, featuremap数, featuremap数,4*アスペクト比の種類数]へ\n",
        "            # （注釈）\n",
        "            # torch.contiguous()はメモリ上で要素を連続的に配置し直す命令です。\n",
        "            # あとでview関数を使用します。\n",
        "            # このviewを行うためには、対象の変数がメモリ上で連続配置されている必要があります。\n",
        "\n",
        "        # さらにlocとconfの形を変形\n",
        "        # locのサイズは、torch.Size([batch_num, 34928])\n",
        "        # confのサイズはtorch.Size([batch_num, 183372])になる\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
        "\n",
        "        # さらにlocとconfの形を整える\n",
        "        # locのサイズは、torch.Size([batch_num, 8732, 4])\n",
        "        # confのサイズは、torch.Size([batch_num, 8732, 21])\n",
        "        loc = loc.view(loc.size(0), -1, 4)\n",
        "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
        "\n",
        "        # 最後に出力する\n",
        "        output = (loc, conf, self.dbox_list)\n",
        "\n",
        "        if self.phase == \"inference\":  # 推論時\n",
        "            # クラス「Detect」のforwardを実行\n",
        "            # 返り値のサイズは torch.Size([batch_num, 21, 200, 5])\n",
        "            return self.detect(output[0], output[1], output[2])\n",
        "\n",
        "        else:  # 学習時\n",
        "            return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxgoGGN9_-86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/phoxter/utils/')\n",
        "from match import match\n",
        "#import importlib\n",
        "#importlib.reload(match)\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSDの損失関数のクラスです。\"\"\"\n",
        "\n",
        "    def __init__(self, jaccard_thresh=0.5, neg_pos=3, device='cpu'):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.jaccard_thresh = jaccard_thresh  # 0.5 関数matchのjaccard係数の閾値\n",
        "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Miningの負と正の比率\n",
        "        self.device = device  # CPUとGPUのいずれで計算するのか\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        損失関数の計算。\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : SSD netの訓練時の出力(tuple)\n",
        "            (loc=torch.Size([num_batch, 8732, 4]), conf=torch.Size([num_batch, 8732, 21]), dbox_list=torch.Size [8732,4])。\n",
        "        targets : [num_batch, num_objs, 5]\n",
        "            5は正解のアノテーション情報[xmin, ymin, xmax, ymax, label_ind]を示す\n",
        "        Returns\n",
        "        -------\n",
        "        loss_l : テンソル\n",
        "            locの損失の値\n",
        "        loss_c : テンソル\n",
        "            confの損失の値\n",
        "        \"\"\"\n",
        "\n",
        "        # SSDモデルの出力がタプルになっているので、個々にばらす\n",
        "        loc_data, conf_data, dbox_list = predictions\n",
        "\n",
        "        # 要素数を把握\n",
        "        num_batch = loc_data.size(0)  # ミニバッチのサイズ\n",
        "        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\n",
        "        num_classes = conf_data.size(2)  # クラス数 = 21\n",
        "\n",
        "        # 損失の計算に使用するものを格納する変数を作成\n",
        "        # conf_t_label：各DBoxに一番近い正解のBBoxのラベルを格納させる\n",
        "        # loc_t:各DBoxに一番近い正解のBBoxの位置情報を格納させる\n",
        "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
        "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
        "\n",
        "        # loc_tとconf_t_labelに、\n",
        "        # DBoxと正解アノテーションtargetsをmatchさせた結果を上書きする\n",
        "        for idx in range(num_batch):  # ミニバッチでループ\n",
        "\n",
        "            # 現在のミニバッチの正解アノテーションのBBoxとラベルを取得\n",
        "            truths = targets[idx][:, :-1].to(self.device)  # BBox\n",
        "            # ラベル [物体1のラベル, 物体2のラベル, …]\n",
        "            labels = targets[idx][:, -1].to(self.device)\n",
        "\n",
        "            # デフォルトボックスを新たな変数で用意\n",
        "            dbox = dbox_list.to(self.device)\n",
        "\n",
        "            # 関数matchを実行し、loc_tとconf_t_labelの内容を更新する\n",
        "            # （詳細）\n",
        "            # loc_t:各DBoxに一番近い正解のBBoxの位置情報が上書きされる\n",
        "            # conf_t_label：各DBoxに一番近いBBoxのラベルが上書きされる\n",
        "            # ただし、一番近いBBoxとのjaccard overlapが0.5より小さい場合は\n",
        "            # 正解BBoxのラベルconf_t_labelは背景クラスの0とする\n",
        "            variance = [0.1, 0.2]\n",
        "            # このvarianceはDBoxからBBoxに補正計算する際に使用する式の係数です\n",
        "            match(self.jaccard_thresh, truths, dbox,\n",
        "                  variance, labels, loc_t, conf_t_label, idx)\n",
        "\n",
        "        # ----------\n",
        "        # 位置の損失：loss_lを計算\n",
        "        # Smooth L1関数で損失を計算する。ただし、物体を発見したDBoxのオフセットのみを計算する\n",
        "        # ----------\n",
        "        # 物体を検出したBBoxを取り出すマスクを作成\n",
        "        pos_mask = conf_t_label > 0  # torch.Size([num_batch, 8732])\n",
        "\n",
        "        # pos_maskをloc_dataのサイズに変形\n",
        "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
        "\n",
        "        # Positive DBoxのloc_dataと、教師データloc_tを取得\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "\n",
        "        # 物体を発見したPositive DBoxのオフセット情報loc_tの損失（誤差）を計算\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "\n",
        "        # ----------\n",
        "        # クラス予測の損失：loss_cを計算\n",
        "        # 交差エントロピー誤差関数で損失を計算する。ただし、背景クラスが正解であるDBoxが圧倒的に多いので、\n",
        "        # Hard Negative Miningを実施し、物体発見DBoxと背景クラスDBoxの比が1:3になるようにする。\n",
        "        # そこで背景クラスDBoxと予想したもののうち、損失が小さいものは、クラス予測の損失から除く\n",
        "        # ----------\n",
        "        batch_conf = conf_data.view(-1, num_classes)\n",
        "\n",
        "        # クラス予測の損失を関数を計算(reduction='none'にして、和をとらず、次元をつぶさない)\n",
        "        loss_c = F.cross_entropy(\n",
        "            batch_conf, conf_t_label.view(-1), reduction='none')\n",
        "\n",
        "        # -----------------\n",
        "        # これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\n",
        "        # -----------------\n",
        "\n",
        "        # 物体発見したPositive DBoxの損失を0にする\n",
        "        # （注意）物体はlabelが1以上になっている。ラベル0は背景。\n",
        "        num_pos = pos_mask.long().sum(1, keepdim=True)  # ミニバッチごとの物体クラス予測の数\n",
        "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
        "        loss_c[pos_mask] = 0  # 物体を発見したDBoxは損失0とする\n",
        "\n",
        "        # Hard Negative Miningを実施する\n",
        "        # 各DBoxの損失の大きさloss_cの順位であるidx_rankを求める\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "\n",
        "        # （注釈）\n",
        "        # 実装コードがかなり特殊で直感的ではないです。\n",
        "        # 上記2行は、要は各DBoxに対して、損失の大きさが何番目なのかの情報を\n",
        "        # 変数idx_rankとして高速に取得したいというコードです。\n",
        "        #\n",
        "        # DBOXの損失値の大きい方から降順に並べ、DBoxの降順のindexをloss_idxに格納。\n",
        "        # 損失の大きさloss_cの順位であるidx_rankを求める。\n",
        "        # ここで、\n",
        "        # 降順になった配列indexであるloss_idxを、0から8732まで昇順に並べ直すためには、\n",
        "        # 何番目のloss_idxのインデックスをとってきたら良いのかを示すのが、idx_rankである。\n",
        "        # 例えば、\n",
        "        # idx_rankの要素0番目 = idx_rank[0]を求めるには、loss_idxの値が0の要素、\n",
        "        # つまりloss_idx[?}=0 の、?は何番かを求めることになる。ここで、? = idx_rank[0]である。\n",
        "        # いま、loss_idx[?]=0の0は、元のloss_cの要素の0番目という意味である。\n",
        "        # つまり?は、元のloss_cの要素0番目は、降順に並び替えられたloss_idxの何番目ですか\n",
        "        # を求めていることになり、 結果、\n",
        "        # ? = idx_rank[0] はloss_cの要素0番目が、降順の何番目かを示すことになる。\n",
        "\n",
        "        # 背景のDBoxの数num_negを決める。HardNegative Miningにより、\n",
        "        # 物体発見のDBoxの数num_posの3倍（self.negpos_ratio倍）とする。\n",
        "        # ただし、万が一、DBoxの数を超える場合は、DBoxの数を上限とする\n",
        "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
        "\n",
        "        # idx_rankは各DBoxの損失の大きさが上から何番目なのかが入っている\n",
        "        # 背景のDBoxの数num_negよりも、順位が低い（すなわち損失が大きい）DBoxを取るマスク作成\n",
        "        # torch.Size([num_batch, 8732])\n",
        "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
        "\n",
        "        # -----------------\n",
        "        # （終了）これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\n",
        "        # -----------------\n",
        "\n",
        "        # マスクの形を整形し、conf_dataに合わせる\n",
        "        # pos_idx_maskはPositive DBoxのconfを取り出すマスクです\n",
        "        # neg_idx_maskはHard Negative Miningで抽出したNegative DBoxのconfを取り出すマスクです\n",
        "        # pos_mask：torch.Size([num_batch, 8732])→pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
        "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
        "\n",
        "        # conf_dataからposとnegだけを取り出してconf_hnmにする。形はtorch.Size([num_pos+num_neg, 21])\n",
        "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\n",
        "                             ].view(-1, num_classes)\n",
        "        # （注釈）gtは greater than (>)の略称。これでmaskが1のindexを取り出す。\n",
        "        # pos_idx_mask+neg_idx_maskは足し算だが、indexへのmaskをまとめているだけである。\n",
        "        # つまり、posであろうがnegであろうが、マスクが1のものを足し算で一つのリストにし、それをgtで取得\n",
        "\n",
        "        # 同様に教師データであるconf_t_labelからposとnegだけを取り出してconf_t_label_hnmに\n",
        "        # 形はtorch.Size([pos+neg])になる\n",
        "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
        "\n",
        "        # confidenceの損失関数を計算（要素の合計=sumを求める）\n",
        "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
        "\n",
        "        # 物体を発見したBBoxの数N（全ミニバッチの合計）で損失を割り算\n",
        "        N = num_pos.sum()\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "\n",
        "        return loss_l, loss_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yifwu-hB_-5g",
        "colab_type": "code",
        "outputId": "89b70d8d-55bb-4107-865a-0a2e4c8feb7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# SSD300の設定\n",
        "ssd_cfg = {\n",
        "    'num_classes': len(classes) + 1,  # 背景クラスを含めた合計クラス数\n",
        "    'input_size': 300,  # 画像の入力サイズ\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "}\n",
        "\n",
        "# SSDネットワークモデル\n",
        "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
        "\n",
        "# SSDの初期の重みを設定\n",
        "# ssdのvgg部分に重みをロードする\n",
        "WEIGHT_DIR = '/content/drive/My Drive/phoxter/weights'\n",
        "vgg_weights = torch.load(WEIGHT_DIR + '/vgg16_reducedfc.pth')\n",
        "net.vgg.load_state_dict(vgg_weights)\n",
        "\n",
        "# ssdのその他のネットワークの重みはHeの初期値で初期化\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "        if m.bias is not None:  # バイアス項がある場合\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "# Heの初期値を適用\n",
        "net.extras.apply(weights_init)\n",
        "net.loc.apply(weights_init)\n",
        "net.conf.apply(weights_init)\n",
        "\n",
        "# GPUが使えるかを確認\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"使用デバイス：\", device)\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "ネットワーク設定完了：学習済みの重みをロードしました\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2I1tZGrAWtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 損失関数の設定\n",
        "criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\n",
        "\n",
        "# 最適化手法の設定\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.0001,\n",
        "                      momentum=0.9, weight_decay=5e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KkmZaaRAWqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # イテレーションカウンタをセット\n",
        "    iteration = 1\n",
        "    epoch_train_loss = 0.0  # epochの損失和\n",
        "    epoch_val_loss = 0.0  # epochの損失和\n",
        "    logs = []\n",
        "    #es = EarlyStopping(patience=10, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs+1):\n",
        "          \n",
        "        # 開始時刻を保存\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "\n",
        "        print('-------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-------------')\n",
        "\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "                print('（train）')\n",
        "            else:\n",
        "                #if((epoch+1) % 10 == 0):\n",
        "                if((epoch+1) % 1 == 0):\n",
        "                    net.eval()   # モデルを検証モードに\n",
        "                    print('-------------')\n",
        "                    print('（val）')\n",
        "                else:\n",
        "                    # 検証は毎回\n",
        "                    continue\n",
        "\n",
        "            # データローダーからminibatchずつ取り出すループ\n",
        "            for images, targets in dataloaders_dict[phase]:\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                images = images.to(device)\n",
        "                targets = [ann.to(device)\n",
        "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # 順伝搬（forward）計算\n",
        "                    outputs = net(images)\n",
        "\n",
        "                    # 損失の計算\n",
        "                    loss_l, loss_c = criterion(outputs, targets)\n",
        "                    loss = loss_l + loss_c\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()  # 勾配の計算\n",
        "\n",
        "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
        "                        nn.utils.clip_grad_value_(\n",
        "                            net.parameters(), clip_value=2.0)\n",
        "\n",
        "                        optimizer.step()  # パラメータ更新\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item(), duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "\n",
        "                    # 検証時\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item()\n",
        "\n",
        "        # epochのphaseごとのlossと正解率\n",
        "        t_epoch_finish = time.time()\n",
        "        print('-------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        # ログを保存\n",
        "        log_epoch = {'epoch': epoch+1,\n",
        "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(path + \"log_output.csv\")\n",
        "\n",
        "        epoch_train_loss = 0.0  # epochの損失和\n",
        "        epoch_val_loss = 0.0  # epochの損失和\n",
        "\n",
        "        # ネットワークを保存する\n",
        "        if ((epoch+1) % 10 == 0):\n",
        "            torch.save(net.state_dict(), WEIGHT_DIR+'/ssd300_' +\n",
        "                       str(epoch+1) + '.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNQLfJnAWm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "0574532b-af17-4c55-9784-ea1ff38fec6d"
      },
      "source": [
        "# Datasetを作成\n",
        "TRAIN_SIZE = int (len(all_img_list) * 0.8 )\n",
        "\n",
        "train_img_list = all_img_list[0:TRAIN_SIZE]\n",
        "val_img_list = all_img_list[TRAIN_SIZE:]\n",
        "\n",
        "print(train_img_list[-1])\n",
        "print(val_img_list[0])\n",
        "print('train size:',len(train_img_list))\n",
        "print('validation size:',len(val_img_list))\n",
        "\n",
        "\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "\n",
        "train_dataset = Dataset(train_img_list, train_img_list,phase=\"train\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_list())\n",
        "\n",
        "val_dataset = Dataset(val_img_list, val_img_list,phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_list())\n",
        "\n",
        "\n",
        "# DataLoaderを作成する\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/phoxter/dot/190903084934_805_001_B1011-27800-2.png\n",
            "/content/drive/My Drive/phoxter/dot/190903084934_805_001_B1011-27800-2.png\n",
            "train size: 4569\n",
            "validation size: 1143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9vU4M6dAWlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d9bd580-cb2f-46d6-b441-b83e69a2585e"
      },
      "source": [
        "# 学習・検証を実行する\n",
        "num_epochs= 50\n",
        "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-------------\n",
            "Epoch 1/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 10 || Loss: 17.8441 || 10iter: 247.3241 sec.\n",
            "イテレーション 20 || Loss: 14.5816 || 10iter: 180.3321 sec.\n",
            "イテレーション 30 || Loss: 13.2425 || 10iter: 142.2520 sec.\n",
            "イテレーション 40 || Loss: 10.8103 || 10iter: 86.7099 sec.\n",
            "イテレーション 50 || Loss: 6.6989 || 10iter: 61.4047 sec.\n",
            "イテレーション 60 || Loss: 6.5160 || 10iter: 43.3605 sec.\n",
            "イテレーション 70 || Loss: 5.9326 || 10iter: 34.4540 sec.\n",
            "イテレーション 80 || Loss: 5.8623 || 10iter: 28.9959 sec.\n",
            "イテレーション 90 || Loss: 5.5620 || 10iter: 36.7506 sec.\n",
            "イテレーション 100 || Loss: 5.5225 || 10iter: 23.6647 sec.\n",
            "イテレーション 110 || Loss: 5.5098 || 10iter: 23.5483 sec.\n",
            "イテレーション 120 || Loss: 4.9242 || 10iter: 23.5035 sec.\n",
            "イテレーション 130 || Loss: 4.6940 || 10iter: 23.6313 sec.\n",
            "イテレーション 140 || Loss: 5.1981 || 10iter: 23.5819 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 1 || Epoch_TRAIN_Loss:1249.1587 ||Epoch_VAL_Loss:294.7111\n",
            "timer:  1212.6480 sec.\n",
            "-------------\n",
            "Epoch 2/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 150 || Loss: 4.8419 || 10iter: 16.0810 sec.\n",
            "イテレーション 160 || Loss: 4.3843 || 10iter: 23.4616 sec.\n",
            "イテレーション 170 || Loss: 4.5586 || 10iter: 23.4476 sec.\n",
            "イテレーション 180 || Loss: 4.2764 || 10iter: 23.6679 sec.\n",
            "イテレーション 190 || Loss: 4.5311 || 10iter: 23.5934 sec.\n",
            "イテレーション 200 || Loss: 4.2902 || 10iter: 23.4924 sec.\n",
            "イテレーション 210 || Loss: 4.2805 || 10iter: 23.4795 sec.\n",
            "イテレーション 220 || Loss: 3.8492 || 10iter: 23.5793 sec.\n",
            "イテレーション 230 || Loss: 3.7273 || 10iter: 23.5154 sec.\n",
            "イテレーション 240 || Loss: 4.2486 || 10iter: 23.7204 sec.\n",
            "イテレーション 250 || Loss: 4.0712 || 10iter: 23.6941 sec.\n",
            "イテレーション 260 || Loss: 3.6301 || 10iter: 23.4502 sec.\n",
            "イテレーション 270 || Loss: 4.1118 || 10iter: 23.4850 sec.\n",
            "イテレーション 280 || Loss: 3.9189 || 10iter: 23.5792 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 2 || Epoch_TRAIN_Loss:605.2812 ||Epoch_VAL_Loss:311.6341\n",
            "timer:  396.0982 sec.\n",
            "-------------\n",
            "Epoch 3/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 290 || Loss: 3.8717 || 10iter: 8.5656 sec.\n",
            "イテレーション 300 || Loss: 3.8337 || 10iter: 23.4597 sec.\n",
            "イテレーション 310 || Loss: 3.7056 || 10iter: 23.5506 sec.\n",
            "イテレーション 320 || Loss: 3.8278 || 10iter: 23.4938 sec.\n",
            "イテレーション 330 || Loss: 3.4665 || 10iter: 23.5663 sec.\n",
            "イテレーション 340 || Loss: 3.7077 || 10iter: 23.6110 sec.\n",
            "イテレーション 350 || Loss: 3.4590 || 10iter: 23.6160 sec.\n",
            "イテレーション 360 || Loss: 3.3712 || 10iter: 23.7509 sec.\n",
            "イテレーション 370 || Loss: 3.8733 || 10iter: 23.4844 sec.\n",
            "イテレーション 380 || Loss: 3.6410 || 10iter: 23.4745 sec.\n",
            "イテレーション 390 || Loss: 3.4380 || 10iter: 23.5235 sec.\n",
            "イテレーション 400 || Loss: 3.5984 || 10iter: 23.6016 sec.\n",
            "イテレーション 410 || Loss: 3.6323 || 10iter: 23.4414 sec.\n",
            "イテレーション 420 || Loss: 3.2227 || 10iter: 23.4738 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 3 || Epoch_TRAIN_Loss:515.7159 ||Epoch_VAL_Loss:326.0736\n",
            "timer:  395.6791 sec.\n",
            "-------------\n",
            "Epoch 4/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 430 || Loss: 3.1954 || 10iter: 1.1908 sec.\n",
            "イテレーション 440 || Loss: 3.3925 || 10iter: 23.5476 sec.\n",
            "イテレーション 450 || Loss: 3.2577 || 10iter: 23.6815 sec.\n",
            "イテレーション 460 || Loss: 3.3388 || 10iter: 23.7291 sec.\n",
            "イテレーション 470 || Loss: 3.1326 || 10iter: 23.7529 sec.\n",
            "イテレーション 480 || Loss: 3.2890 || 10iter: 23.6331 sec.\n",
            "イテレーション 490 || Loss: 3.4156 || 10iter: 23.7187 sec.\n",
            "イテレーション 500 || Loss: 3.0038 || 10iter: 23.6920 sec.\n",
            "イテレーション 510 || Loss: 3.4128 || 10iter: 23.5020 sec.\n",
            "イテレーション 520 || Loss: 3.2558 || 10iter: 23.5313 sec.\n",
            "イテレーション 530 || Loss: 3.3205 || 10iter: 23.5939 sec.\n",
            "イテレーション 540 || Loss: 3.2028 || 10iter: 23.5284 sec.\n",
            "イテレーション 550 || Loss: 3.2154 || 10iter: 23.5972 sec.\n",
            "イテレーション 560 || Loss: 3.2188 || 10iter: 23.5656 sec.\n",
            "イテレーション 570 || Loss: 3.4481 || 10iter: 23.5775 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 4 || Epoch_TRAIN_Loss:469.6137 ||Epoch_VAL_Loss:338.9994\n",
            "timer:  397.3733 sec.\n",
            "-------------\n",
            "Epoch 5/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 580 || Loss: 3.0497 || 10iter: 18.6752 sec.\n",
            "イテレーション 590 || Loss: 3.3302 || 10iter: 23.4129 sec.\n",
            "イテレーション 600 || Loss: 3.1113 || 10iter: 23.5136 sec.\n",
            "イテレーション 610 || Loss: 3.2027 || 10iter: 23.4822 sec.\n",
            "イテレーション 620 || Loss: 3.3470 || 10iter: 23.6261 sec.\n",
            "イテレーション 630 || Loss: 3.1099 || 10iter: 23.5465 sec.\n",
            "イテレーション 640 || Loss: 3.1830 || 10iter: 23.5541 sec.\n",
            "イテレーション 650 || Loss: 3.0185 || 10iter: 23.5131 sec.\n",
            "イテレーション 660 || Loss: 3.0334 || 10iter: 23.5428 sec.\n",
            "イテレーション 670 || Loss: 3.1609 || 10iter: 23.5095 sec.\n",
            "イテレーション 680 || Loss: 3.0784 || 10iter: 23.4689 sec.\n",
            "イテレーション 690 || Loss: 3.0244 || 10iter: 23.6725 sec.\n",
            "イテレーション 700 || Loss: 3.1406 || 10iter: 23.6675 sec.\n",
            "イテレーション 710 || Loss: 3.0117 || 10iter: 23.6080 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 5 || Epoch_TRAIN_Loss:444.0738 ||Epoch_VAL_Loss:343.4489\n",
            "timer:  396.3206 sec.\n",
            "-------------\n",
            "Epoch 6/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 720 || Loss: 3.0031 || 10iter: 11.1884 sec.\n",
            "イテレーション 730 || Loss: 3.0059 || 10iter: 23.5566 sec.\n",
            "イテレーション 740 || Loss: 2.8709 || 10iter: 23.6197 sec.\n",
            "イテレーション 750 || Loss: 2.9117 || 10iter: 23.6058 sec.\n",
            "イテレーション 760 || Loss: 2.7769 || 10iter: 23.6834 sec.\n",
            "イテレーション 770 || Loss: 2.9427 || 10iter: 23.5937 sec.\n",
            "イテレーション 780 || Loss: 3.1285 || 10iter: 23.6056 sec.\n",
            "イテレーション 790 || Loss: 3.0214 || 10iter: 23.7205 sec.\n",
            "イテレーション 800 || Loss: 3.0426 || 10iter: 23.6361 sec.\n",
            "イテレーション 810 || Loss: 2.8643 || 10iter: 23.8750 sec.\n",
            "イテレーション 820 || Loss: 2.9350 || 10iter: 23.6156 sec.\n",
            "イテレーション 830 || Loss: 2.9775 || 10iter: 23.6056 sec.\n",
            "イテレーション 840 || Loss: 2.9923 || 10iter: 23.5850 sec.\n",
            "イテレーション 850 || Loss: 2.9742 || 10iter: 23.7994 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 6 || Epoch_TRAIN_Loss:427.7897 ||Epoch_VAL_Loss:350.5306\n",
            "timer:  398.2325 sec.\n",
            "-------------\n",
            "Epoch 7/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 860 || Loss: 2.8876 || 10iter: 3.7121 sec.\n",
            "イテレーション 870 || Loss: 2.9226 || 10iter: 23.6577 sec.\n",
            "イテレーション 880 || Loss: 3.0202 || 10iter: 23.6954 sec.\n",
            "イテレーション 890 || Loss: 2.8902 || 10iter: 23.6702 sec.\n",
            "イテレーション 900 || Loss: 2.9342 || 10iter: 23.8809 sec.\n",
            "イテレーション 910 || Loss: 2.7552 || 10iter: 23.6778 sec.\n",
            "イテレーション 920 || Loss: 2.7965 || 10iter: 23.9628 sec.\n",
            "イテレーション 930 || Loss: 2.8684 || 10iter: 23.6842 sec.\n",
            "イテレーション 940 || Loss: 2.8533 || 10iter: 23.7334 sec.\n",
            "イテレーション 950 || Loss: 2.9093 || 10iter: 23.6725 sec.\n",
            "イテレーション 960 || Loss: 2.9981 || 10iter: 23.6914 sec.\n",
            "イテレーション 970 || Loss: 2.8063 || 10iter: 23.7794 sec.\n",
            "イテレーション 980 || Loss: 2.8260 || 10iter: 23.6927 sec.\n",
            "イテレーション 990 || Loss: 2.8764 || 10iter: 23.7263 sec.\n",
            "イテレーション 1000 || Loss: 2.8298 || 10iter: 23.8290 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 7 || Epoch_TRAIN_Loss:415.5781 ||Epoch_VAL_Loss:354.5886\n",
            "timer:  399.7829 sec.\n",
            "-------------\n",
            "Epoch 8/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1010 || Loss: 2.8272 || 10iter: 21.2676 sec.\n",
            "イテレーション 1020 || Loss: 2.6777 || 10iter: 23.7824 sec.\n",
            "イテレーション 1030 || Loss: 2.8926 || 10iter: 23.7442 sec.\n",
            "イテレーション 1040 || Loss: 2.8447 || 10iter: 23.7854 sec.\n",
            "イテレーション 1050 || Loss: 2.7352 || 10iter: 23.7099 sec.\n",
            "イテレーション 1060 || Loss: 2.7816 || 10iter: 23.6459 sec.\n",
            "イテレーション 1070 || Loss: 2.8957 || 10iter: 23.7141 sec.\n",
            "イテレーション 1080 || Loss: 2.8794 || 10iter: 23.8100 sec.\n",
            "イテレーション 1090 || Loss: 2.7755 || 10iter: 23.7480 sec.\n",
            "イテレーション 1100 || Loss: 2.8706 || 10iter: 23.8386 sec.\n",
            "イテレーション 1110 || Loss: 2.7646 || 10iter: 23.6779 sec.\n",
            "イテレーション 1120 || Loss: 2.9169 || 10iter: 23.8382 sec.\n",
            "イテレーション 1130 || Loss: 2.8717 || 10iter: 23.6512 sec.\n",
            "イテレーション 1140 || Loss: 2.8602 || 10iter: 23.7192 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 8 || Epoch_TRAIN_Loss:407.4144 ||Epoch_VAL_Loss:356.9798\n",
            "timer:  399.5284 sec.\n",
            "-------------\n",
            "Epoch 9/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1150 || Loss: 2.8038 || 10iter: 13.6693 sec.\n",
            "イテレーション 1160 || Loss: 2.9334 || 10iter: 23.5302 sec.\n",
            "イテレーション 1170 || Loss: 2.8948 || 10iter: 23.6260 sec.\n",
            "イテレーション 1180 || Loss: 2.8411 || 10iter: 23.5105 sec.\n",
            "イテレーション 1190 || Loss: 2.7090 || 10iter: 23.5232 sec.\n",
            "イテレーション 1200 || Loss: 2.6975 || 10iter: 23.5865 sec.\n",
            "イテレーション 1210 || Loss: 2.8508 || 10iter: 23.5560 sec.\n",
            "イテレーション 1220 || Loss: 2.7916 || 10iter: 23.5600 sec.\n",
            "イテレーション 1230 || Loss: 2.7143 || 10iter: 23.6759 sec.\n",
            "イテレーション 1240 || Loss: 2.9346 || 10iter: 23.3315 sec.\n",
            "イテレーション 1250 || Loss: 2.7949 || 10iter: 23.5021 sec.\n",
            "イテレーション 1260 || Loss: 2.6943 || 10iter: 23.7280 sec.\n",
            "イテレーション 1270 || Loss: 2.8526 || 10iter: 23.5363 sec.\n",
            "イテレーション 1280 || Loss: 2.8236 || 10iter: 23.4812 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 9 || Epoch_TRAIN_Loss:400.4404 ||Epoch_VAL_Loss:361.3702\n",
            "timer:  395.9167 sec.\n",
            "-------------\n",
            "Epoch 10/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1290 || Loss: 2.7562 || 10iter: 6.1234 sec.\n",
            "イテレーション 1300 || Loss: 3.0135 || 10iter: 23.4851 sec.\n",
            "イテレーション 1310 || Loss: 2.6818 || 10iter: 23.4294 sec.\n",
            "イテレーション 1320 || Loss: 2.7048 || 10iter: 23.5436 sec.\n",
            "イテレーション 1330 || Loss: 2.7460 || 10iter: 23.4478 sec.\n",
            "イテレーション 1340 || Loss: 2.6463 || 10iter: 23.7155 sec.\n",
            "イテレーション 1350 || Loss: 2.6737 || 10iter: 23.5969 sec.\n",
            "イテレーション 1360 || Loss: 2.8165 || 10iter: 23.5768 sec.\n",
            "イテレーション 1370 || Loss: 2.7849 || 10iter: 23.8538 sec.\n",
            "イテレーション 1380 || Loss: 2.7054 || 10iter: 23.5138 sec.\n",
            "イテレーション 1390 || Loss: 2.5465 || 10iter: 23.5420 sec.\n",
            "イテレーション 1400 || Loss: 2.6362 || 10iter: 23.5211 sec.\n",
            "イテレーション 1410 || Loss: 2.8437 || 10iter: 23.5254 sec.\n",
            "イテレーション 1420 || Loss: 2.7696 || 10iter: 23.6423 sec.\n",
            "イテレーション 1430 || Loss: 2.6112 || 10iter: 23.3105 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 10 || Epoch_TRAIN_Loss:396.1256 ||Epoch_VAL_Loss:361.1236\n",
            "timer:  396.4645 sec.\n",
            "-------------\n",
            "Epoch 11/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1440 || Loss: 2.7530 || 10iter: 23.7311 sec.\n",
            "イテレーション 1450 || Loss: 2.7222 || 10iter: 23.6207 sec.\n",
            "イテレーション 1460 || Loss: 2.7426 || 10iter: 23.5776 sec.\n",
            "イテレーション 1470 || Loss: 2.7976 || 10iter: 23.6477 sec.\n",
            "イテレーション 1480 || Loss: 2.6205 || 10iter: 23.6979 sec.\n",
            "イテレーション 1490 || Loss: 2.7591 || 10iter: 23.6148 sec.\n",
            "イテレーション 1500 || Loss: 2.6804 || 10iter: 23.5891 sec.\n",
            "イテレーション 1510 || Loss: 2.7331 || 10iter: 23.6439 sec.\n",
            "イテレーション 1520 || Loss: 2.7862 || 10iter: 23.5539 sec.\n",
            "イテレーション 1530 || Loss: 2.7669 || 10iter: 23.6253 sec.\n",
            "イテレーション 1540 || Loss: 2.5872 || 10iter: 23.7176 sec.\n",
            "イテレーション 1550 || Loss: 2.8158 || 10iter: 23.6581 sec.\n",
            "イテレーション 1560 || Loss: 2.7867 || 10iter: 23.5979 sec.\n",
            "イテレーション 1570 || Loss: 2.7899 || 10iter: 23.7650 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 11 || Epoch_TRAIN_Loss:391.5125 ||Epoch_VAL_Loss:365.7664\n",
            "timer:  397.8244 sec.\n",
            "-------------\n",
            "Epoch 12/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1580 || Loss: 2.8200 || 10iter: 16.3428 sec.\n",
            "イテレーション 1590 || Loss: 2.6371 || 10iter: 23.6784 sec.\n",
            "イテレーション 1600 || Loss: 2.6762 || 10iter: 23.6721 sec.\n",
            "イテレーション 1610 || Loss: 2.7081 || 10iter: 23.7569 sec.\n",
            "イテレーション 1620 || Loss: 2.6651 || 10iter: 23.6232 sec.\n",
            "イテレーション 1630 || Loss: 2.6912 || 10iter: 23.6005 sec.\n",
            "イテレーション 1640 || Loss: 2.7621 || 10iter: 23.6719 sec.\n",
            "イテレーション 1650 || Loss: 2.6347 || 10iter: 23.7680 sec.\n",
            "イテレーション 1660 || Loss: 2.6750 || 10iter: 23.6858 sec.\n",
            "イテレーション 1670 || Loss: 2.7011 || 10iter: 23.7908 sec.\n",
            "イテレーション 1680 || Loss: 2.6843 || 10iter: 23.6622 sec.\n",
            "イテレーション 1690 || Loss: 2.8091 || 10iter: 23.6657 sec.\n",
            "イテレーション 1700 || Loss: 2.7320 || 10iter: 23.6139 sec.\n",
            "イテレーション 1710 || Loss: 2.8027 || 10iter: 23.8446 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 12 || Epoch_TRAIN_Loss:387.5706 ||Epoch_VAL_Loss:364.2286\n",
            "timer:  398.7436 sec.\n",
            "-------------\n",
            "Epoch 13/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1720 || Loss: 2.6988 || 10iter: 8.6365 sec.\n",
            "イテレーション 1730 || Loss: 2.6645 || 10iter: 23.6360 sec.\n",
            "イテレーション 1740 || Loss: 2.6571 || 10iter: 23.7330 sec.\n",
            "イテレーション 1750 || Loss: 2.7543 || 10iter: 23.6148 sec.\n",
            "イテレーション 1760 || Loss: 2.5774 || 10iter: 23.6233 sec.\n",
            "イテレーション 1770 || Loss: 2.6728 || 10iter: 23.5799 sec.\n",
            "イテレーション 1780 || Loss: 2.6472 || 10iter: 23.6342 sec.\n",
            "イテレーション 1790 || Loss: 2.5901 || 10iter: 23.6301 sec.\n",
            "イテレーション 1800 || Loss: 2.6377 || 10iter: 23.6039 sec.\n",
            "イテレーション 1810 || Loss: 2.6804 || 10iter: 23.7144 sec.\n",
            "イテレーション 1820 || Loss: 2.6459 || 10iter: 23.7557 sec.\n",
            "イテレーション 1830 || Loss: 2.6395 || 10iter: 23.5877 sec.\n",
            "イテレーション 1840 || Loss: 2.9407 || 10iter: 23.6387 sec.\n",
            "イテレーション 1850 || Loss: 2.6020 || 10iter: 23.6360 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 13 || Epoch_TRAIN_Loss:383.0836 ||Epoch_VAL_Loss:372.4430\n",
            "timer:  397.8286 sec.\n",
            "-------------\n",
            "Epoch 14/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1860 || Loss: 2.6686 || 10iter: 1.2243 sec.\n",
            "イテレーション 1870 || Loss: 2.6308 || 10iter: 23.6373 sec.\n",
            "イテレーション 1880 || Loss: 2.7515 || 10iter: 23.7188 sec.\n",
            "イテレーション 1890 || Loss: 2.6152 || 10iter: 23.5530 sec.\n",
            "イテレーション 1900 || Loss: 2.7998 || 10iter: 23.5791 sec.\n",
            "イテレーション 1910 || Loss: 2.6651 || 10iter: 23.7132 sec.\n",
            "イテレーション 1920 || Loss: 2.6159 || 10iter: 23.7523 sec.\n",
            "イテレーション 1930 || Loss: 2.6274 || 10iter: 23.6666 sec.\n",
            "イテレーション 1940 || Loss: 2.6648 || 10iter: 23.5764 sec.\n",
            "イテレーション 1950 || Loss: 2.7371 || 10iter: 23.6672 sec.\n",
            "イテレーション 1960 || Loss: 2.6674 || 10iter: 23.5813 sec.\n",
            "イテレーション 1970 || Loss: 2.6333 || 10iter: 23.6390 sec.\n",
            "イテレーション 1980 || Loss: 2.6096 || 10iter: 23.6028 sec.\n",
            "イテレーション 1990 || Loss: 2.6236 || 10iter: 23.7023 sec.\n",
            "イテレーション 2000 || Loss: 2.6292 || 10iter: 23.6700 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 14 || Epoch_TRAIN_Loss:380.8006 ||Epoch_VAL_Loss:371.0875\n",
            "timer:  398.1581 sec.\n",
            "-------------\n",
            "Epoch 15/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2010 || Loss: 2.6233 || 10iter: 18.6477 sec.\n",
            "イテレーション 2020 || Loss: 2.6422 || 10iter: 23.5277 sec.\n",
            "イテレーション 2030 || Loss: 2.8346 || 10iter: 23.8636 sec.\n",
            "イテレーション 2040 || Loss: 2.6973 || 10iter: 23.6443 sec.\n",
            "イテレーション 2050 || Loss: 2.7288 || 10iter: 23.6869 sec.\n",
            "イテレーション 2060 || Loss: 2.7101 || 10iter: 23.6635 sec.\n",
            "イテレーション 2070 || Loss: 2.7041 || 10iter: 23.6803 sec.\n",
            "イテレーション 2080 || Loss: 2.6363 || 10iter: 23.6643 sec.\n",
            "イテレーション 2090 || Loss: 2.6326 || 10iter: 23.6480 sec.\n",
            "イテレーション 2100 || Loss: 2.5765 || 10iter: 23.7078 sec.\n",
            "イテレーション 2110 || Loss: 2.6699 || 10iter: 23.7368 sec.\n",
            "イテレーション 2120 || Loss: 2.9525 || 10iter: 23.6489 sec.\n",
            "イテレーション 2130 || Loss: 2.5760 || 10iter: 23.5290 sec.\n",
            "イテレーション 2140 || Loss: 2.5826 || 10iter: 23.6121 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 15 || Epoch_TRAIN_Loss:379.3509 ||Epoch_VAL_Loss:370.7733\n",
            "timer:  398.4407 sec.\n",
            "-------------\n",
            "Epoch 16/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2150 || Loss: 2.6281 || 10iter: 11.1992 sec.\n",
            "イテレーション 2160 || Loss: 2.5876 || 10iter: 23.5912 sec.\n",
            "イテレーション 2170 || Loss: 2.6428 || 10iter: 23.6193 sec.\n",
            "イテレーション 2180 || Loss: 2.6221 || 10iter: 23.6438 sec.\n",
            "イテレーション 2190 || Loss: 2.6302 || 10iter: 23.6785 sec.\n",
            "イテレーション 2200 || Loss: 2.6284 || 10iter: 23.4901 sec.\n",
            "イテレーション 2210 || Loss: 2.6737 || 10iter: 23.5201 sec.\n",
            "イテレーション 2220 || Loss: 2.6040 || 10iter: 23.7371 sec.\n",
            "イテレーション 2230 || Loss: 2.6716 || 10iter: 23.6127 sec.\n",
            "イテレーション 2240 || Loss: 2.5789 || 10iter: 23.6642 sec.\n",
            "イテレーション 2250 || Loss: 2.6446 || 10iter: 23.6403 sec.\n",
            "イテレーション 2260 || Loss: 2.5260 || 10iter: 23.7612 sec.\n",
            "イテレーション 2270 || Loss: 2.6057 || 10iter: 23.5967 sec.\n",
            "イテレーション 2280 || Loss: 2.5477 || 10iter: 23.5911 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 16 || Epoch_TRAIN_Loss:376.9960 ||Epoch_VAL_Loss:372.2132\n",
            "timer:  397.7069 sec.\n",
            "-------------\n",
            "Epoch 17/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2290 || Loss: 2.8689 || 10iter: 3.7069 sec.\n",
            "イテレーション 2300 || Loss: 2.6111 || 10iter: 23.6333 sec.\n",
            "イテレーション 2310 || Loss: 2.5167 || 10iter: 23.6006 sec.\n",
            "イテレーション 2320 || Loss: 2.5210 || 10iter: 23.7773 sec.\n",
            "イテレーション 2330 || Loss: 2.6032 || 10iter: 23.6476 sec.\n",
            "イテレーション 2340 || Loss: 2.5794 || 10iter: 23.6852 sec.\n",
            "イテレーション 2350 || Loss: 2.5722 || 10iter: 23.6012 sec.\n",
            "イテレーション 2360 || Loss: 2.6539 || 10iter: 23.5966 sec.\n",
            "イテレーション 2370 || Loss: 2.6258 || 10iter: 23.7941 sec.\n",
            "イテレーション 2380 || Loss: 2.5660 || 10iter: 23.5817 sec.\n",
            "イテレーション 2390 || Loss: 2.6373 || 10iter: 23.6622 sec.\n",
            "イテレーション 2400 || Loss: 2.5710 || 10iter: 23.5964 sec.\n",
            "イテレーション 2410 || Loss: 2.5685 || 10iter: 23.6083 sec.\n",
            "イテレーション 2420 || Loss: 2.5205 || 10iter: 23.6249 sec.\n",
            "イテレーション 2430 || Loss: 2.6835 || 10iter: 23.6717 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 17 || Epoch_TRAIN_Loss:373.9710 ||Epoch_VAL_Loss:374.6321\n",
            "timer:  397.9264 sec.\n",
            "-------------\n",
            "Epoch 18/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2440 || Loss: 2.6117 || 10iter: 21.1200 sec.\n",
            "イテレーション 2450 || Loss: 2.5797 || 10iter: 23.3613 sec.\n",
            "イテレーション 2460 || Loss: 2.5313 || 10iter: 23.3871 sec.\n",
            "イテレーション 2470 || Loss: 2.5691 || 10iter: 23.4711 sec.\n",
            "イテレーション 2480 || Loss: 2.6030 || 10iter: 23.5678 sec.\n",
            "イテレーション 2490 || Loss: 2.5769 || 10iter: 23.5294 sec.\n",
            "イテレーション 2500 || Loss: 2.6315 || 10iter: 23.5812 sec.\n",
            "イテレーション 2510 || Loss: 2.5764 || 10iter: 23.6054 sec.\n",
            "イテレーション 2520 || Loss: 2.5517 || 10iter: 23.6326 sec.\n",
            "イテレーション 2530 || Loss: 2.5758 || 10iter: 23.5460 sec.\n",
            "イテレーション 2540 || Loss: 2.5364 || 10iter: 23.7415 sec.\n",
            "イテレーション 2550 || Loss: 2.6147 || 10iter: 23.6187 sec.\n",
            "イテレーション 2560 || Loss: 2.5829 || 10iter: 23.5268 sec.\n",
            "イテレーション 2570 || Loss: 2.6177 || 10iter: 23.6896 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 18 || Epoch_TRAIN_Loss:373.6799 ||Epoch_VAL_Loss:374.8880\n",
            "timer:  396.8221 sec.\n",
            "-------------\n",
            "Epoch 19/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2580 || Loss: 2.5806 || 10iter: 13.7487 sec.\n",
            "イテレーション 2590 || Loss: 2.4877 || 10iter: 23.8081 sec.\n",
            "イテレーション 2600 || Loss: 2.4826 || 10iter: 23.6068 sec.\n",
            "イテレーション 2610 || Loss: 2.5637 || 10iter: 23.6953 sec.\n",
            "イテレーション 2620 || Loss: 2.6367 || 10iter: 23.6216 sec.\n",
            "イテレーション 2630 || Loss: 2.6024 || 10iter: 23.6453 sec.\n",
            "イテレーション 2640 || Loss: 2.5792 || 10iter: 23.6580 sec.\n",
            "イテレーション 2650 || Loss: 2.5444 || 10iter: 23.6882 sec.\n",
            "イテレーション 2660 || Loss: 2.5391 || 10iter: 23.7790 sec.\n",
            "イテレーション 2670 || Loss: 2.5899 || 10iter: 23.7184 sec.\n",
            "イテレーション 2680 || Loss: 2.5831 || 10iter: 23.6982 sec.\n",
            "イテレーション 2690 || Loss: 2.5762 || 10iter: 23.6982 sec.\n",
            "イテレーション 2700 || Loss: 2.5360 || 10iter: 23.6746 sec.\n",
            "イテレーション 2710 || Loss: 2.6820 || 10iter: 23.8214 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 19 || Epoch_TRAIN_Loss:370.9496 ||Epoch_VAL_Loss:375.6449\n",
            "timer:  398.6271 sec.\n",
            "-------------\n",
            "Epoch 20/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2720 || Loss: 2.5824 || 10iter: 6.2036 sec.\n",
            "イテレーション 2730 || Loss: 2.6133 || 10iter: 23.7046 sec.\n",
            "イテレーション 2740 || Loss: 2.5481 || 10iter: 23.6588 sec.\n",
            "イテレーション 2750 || Loss: 2.6389 || 10iter: 23.6682 sec.\n",
            "イテレーション 2760 || Loss: 2.5297 || 10iter: 23.7825 sec.\n",
            "イテレーション 2770 || Loss: 2.5647 || 10iter: 23.7157 sec.\n",
            "イテレーション 2780 || Loss: 2.5548 || 10iter: 23.6951 sec.\n",
            "イテレーション 2790 || Loss: 2.5356 || 10iter: 23.7128 sec.\n",
            "イテレーション 2800 || Loss: 2.5943 || 10iter: 23.7782 sec.\n",
            "イテレーション 2810 || Loss: 2.6702 || 10iter: 23.6558 sec.\n",
            "イテレーション 2820 || Loss: 2.6480 || 10iter: 23.8953 sec.\n",
            "イテレーション 2830 || Loss: 2.5396 || 10iter: 23.6395 sec.\n",
            "イテレーション 2840 || Loss: 2.6355 || 10iter: 23.6893 sec.\n",
            "イテレーション 2850 || Loss: 2.6246 || 10iter: 23.6099 sec.\n",
            "イテレーション 2860 || Loss: 2.6195 || 10iter: 23.4109 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 20 || Epoch_TRAIN_Loss:369.4820 ||Epoch_VAL_Loss:379.6412\n",
            "timer:  398.7027 sec.\n",
            "-------------\n",
            "Epoch 21/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2870 || Loss: 2.5144 || 10iter: 23.8838 sec.\n",
            "イテレーション 2880 || Loss: 2.6409 || 10iter: 23.6782 sec.\n",
            "イテレーション 2890 || Loss: 2.6708 || 10iter: 23.6791 sec.\n",
            "イテレーション 2900 || Loss: 2.5815 || 10iter: 23.7141 sec.\n",
            "イテレーション 2910 || Loss: 2.5257 || 10iter: 23.6620 sec.\n",
            "イテレーション 2920 || Loss: 2.7111 || 10iter: 23.6472 sec.\n",
            "イテレーション 2930 || Loss: 2.4867 || 10iter: 23.8717 sec.\n",
            "イテレーション 2940 || Loss: 2.6428 || 10iter: 23.6027 sec.\n",
            "イテレーション 2950 || Loss: 2.5396 || 10iter: 23.7116 sec.\n",
            "イテレーション 2960 || Loss: 2.6592 || 10iter: 23.6460 sec.\n",
            "イテレーション 2970 || Loss: 2.5583 || 10iter: 23.6766 sec.\n",
            "イテレーション 2980 || Loss: 2.5328 || 10iter: 23.6297 sec.\n",
            "イテレーション 2990 || Loss: 2.5353 || 10iter: 23.7263 sec.\n",
            "イテレーション 3000 || Loss: 2.6160 || 10iter: 23.6476 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 21 || Epoch_TRAIN_Loss:370.1975 ||Epoch_VAL_Loss:380.6751\n",
            "timer:  398.6760 sec.\n",
            "-------------\n",
            "Epoch 22/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3010 || Loss: 2.5381 || 10iter: 16.1444 sec.\n",
            "イテレーション 3020 || Loss: 2.4582 || 10iter: 23.6457 sec.\n",
            "イテレーション 3030 || Loss: 2.5521 || 10iter: 23.7239 sec.\n",
            "イテレーション 3040 || Loss: 2.5903 || 10iter: 23.7808 sec.\n",
            "イテレーション 3050 || Loss: 2.4585 || 10iter: 23.6007 sec.\n",
            "イテレーション 3060 || Loss: 2.5169 || 10iter: 23.7071 sec.\n",
            "イテレーション 3070 || Loss: 2.5611 || 10iter: 23.6401 sec.\n",
            "イテレーション 3080 || Loss: 2.5433 || 10iter: 23.6578 sec.\n",
            "イテレーション 3090 || Loss: 2.5694 || 10iter: 23.5672 sec.\n",
            "イテレーション 3100 || Loss: 2.5401 || 10iter: 23.8508 sec.\n",
            "イテレーション 3110 || Loss: 2.6172 || 10iter: 23.6365 sec.\n",
            "イテレーション 3120 || Loss: 2.5862 || 10iter: 23.6195 sec.\n",
            "イテレーション 3130 || Loss: 2.6878 || 10iter: 23.6571 sec.\n",
            "イテレーション 3140 || Loss: 2.5704 || 10iter: 23.7009 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 22 || Epoch_TRAIN_Loss:368.5650 ||Epoch_VAL_Loss:381.5639\n",
            "timer:  398.5867 sec.\n",
            "-------------\n",
            "Epoch 23/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3150 || Loss: 2.5233 || 10iter: 8.6492 sec.\n",
            "イテレーション 3160 || Loss: 2.5966 || 10iter: 23.6777 sec.\n",
            "イテレーション 3170 || Loss: 2.5525 || 10iter: 23.6125 sec.\n",
            "イテレーション 3180 || Loss: 2.4889 || 10iter: 23.7506 sec.\n",
            "イテレーション 3190 || Loss: 2.5497 || 10iter: 23.5812 sec.\n",
            "イテレーション 3200 || Loss: 2.6058 || 10iter: 23.6776 sec.\n",
            "イテレーション 3210 || Loss: 2.5877 || 10iter: 23.6470 sec.\n",
            "イテレーション 3220 || Loss: 2.5372 || 10iter: 23.6202 sec.\n",
            "イテレーション 3230 || Loss: 2.5579 || 10iter: 23.6459 sec.\n",
            "イテレーション 3240 || Loss: 2.6242 || 10iter: 23.6576 sec.\n",
            "イテレーション 3250 || Loss: 2.5101 || 10iter: 23.6106 sec.\n",
            "イテレーション 3260 || Loss: 2.5712 || 10iter: 23.6229 sec.\n",
            "イテレーション 3270 || Loss: 2.5372 || 10iter: 23.7746 sec.\n",
            "イテレーション 3280 || Loss: 2.5530 || 10iter: 23.6668 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 23 || Epoch_TRAIN_Loss:366.0586 ||Epoch_VAL_Loss:382.9004\n",
            "timer:  397.9376 sec.\n",
            "-------------\n",
            "Epoch 24/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3290 || Loss: 2.7161 || 10iter: 1.1916 sec.\n",
            "イテレーション 3300 || Loss: 2.5963 || 10iter: 23.6209 sec.\n",
            "イテレーション 3310 || Loss: 2.5865 || 10iter: 23.6608 sec.\n",
            "イテレーション 3320 || Loss: 2.5482 || 10iter: 23.5615 sec.\n",
            "イテレーション 3330 || Loss: 2.5403 || 10iter: 23.7709 sec.\n",
            "イテレーション 3340 || Loss: 2.5344 || 10iter: 23.6651 sec.\n",
            "イテレーション 3350 || Loss: 2.5464 || 10iter: 23.6053 sec.\n",
            "イテレーション 3360 || Loss: 2.5930 || 10iter: 23.7197 sec.\n",
            "イテレーション 3370 || Loss: 2.5695 || 10iter: 23.7407 sec.\n",
            "イテレーション 3380 || Loss: 2.4982 || 10iter: 23.8534 sec.\n",
            "イテレーション 3390 || Loss: 2.5933 || 10iter: 23.6355 sec.\n",
            "イテレーション 3400 || Loss: 2.6328 || 10iter: 23.6598 sec.\n",
            "イテレーション 3410 || Loss: 2.4865 || 10iter: 23.6059 sec.\n",
            "イテレーション 3420 || Loss: 2.6401 || 10iter: 23.6211 sec.\n",
            "イテレーション 3430 || Loss: 2.5668 || 10iter: 23.7431 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 24 || Epoch_TRAIN_Loss:365.4198 ||Epoch_VAL_Loss:382.3280\n",
            "timer:  398.3561 sec.\n",
            "-------------\n",
            "Epoch 25/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3440 || Loss: 2.7543 || 10iter: 18.7569 sec.\n",
            "イテレーション 3450 || Loss: 2.4482 || 10iter: 23.7324 sec.\n",
            "イテレーション 3460 || Loss: 2.5166 || 10iter: 23.6759 sec.\n",
            "イテレーション 3470 || Loss: 2.6041 || 10iter: 23.7363 sec.\n",
            "イテレーション 3480 || Loss: 2.5376 || 10iter: 23.8806 sec.\n",
            "イテレーション 3490 || Loss: 2.5415 || 10iter: 23.7883 sec.\n",
            "イテレーション 3500 || Loss: 2.7274 || 10iter: 23.8351 sec.\n",
            "イテレーション 3510 || Loss: 2.5057 || 10iter: 23.7101 sec.\n",
            "イテレーション 3520 || Loss: 2.5748 || 10iter: 23.7455 sec.\n",
            "イテレーション 3530 || Loss: 2.5251 || 10iter: 23.7269 sec.\n",
            "イテレーション 3540 || Loss: 2.5120 || 10iter: 23.8426 sec.\n",
            "イテレーション 3550 || Loss: 2.5676 || 10iter: 23.6664 sec.\n",
            "イテレーション 3560 || Loss: 2.5305 || 10iter: 23.8192 sec.\n",
            "イテレーション 3570 || Loss: 2.5052 || 10iter: 23.7842 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 25 || Epoch_TRAIN_Loss:363.3211 ||Epoch_VAL_Loss:382.1698\n",
            "timer:  399.6742 sec.\n",
            "-------------\n",
            "Epoch 26/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3580 || Loss: 2.5145 || 10iter: 11.2328 sec.\n",
            "イテレーション 3590 || Loss: 2.5437 || 10iter: 23.8750 sec.\n",
            "イテレーション 3600 || Loss: 2.5381 || 10iter: 23.6275 sec.\n",
            "イテレーション 3610 || Loss: 2.5351 || 10iter: 23.6750 sec.\n",
            "イテレーション 3620 || Loss: 2.7455 || 10iter: 23.6760 sec.\n",
            "イテレーション 3630 || Loss: 2.4625 || 10iter: 23.7503 sec.\n",
            "イテレーション 3640 || Loss: 2.5387 || 10iter: 23.7429 sec.\n",
            "イテレーション 3650 || Loss: 2.5156 || 10iter: 23.7428 sec.\n",
            "イテレーション 3660 || Loss: 2.6514 || 10iter: 23.6300 sec.\n",
            "イテレーション 3670 || Loss: 2.5432 || 10iter: 23.7642 sec.\n",
            "イテレーション 3680 || Loss: 2.5538 || 10iter: 23.7236 sec.\n",
            "イテレーション 3690 || Loss: 2.5920 || 10iter: 23.7340 sec.\n",
            "イテレーション 3700 || Loss: 2.5958 || 10iter: 23.6307 sec.\n",
            "イテレーション 3710 || Loss: 2.4855 || 10iter: 23.7701 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 26 || Epoch_TRAIN_Loss:363.8199 ||Epoch_VAL_Loss:385.8352\n",
            "timer:  399.1662 sec.\n",
            "-------------\n",
            "Epoch 27/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3720 || Loss: 2.4876 || 10iter: 3.7072 sec.\n",
            "イテレーション 3730 || Loss: 2.5035 || 10iter: 23.6267 sec.\n",
            "イテレーション 3740 || Loss: 2.4797 || 10iter: 23.7973 sec.\n",
            "イテレーション 3750 || Loss: 2.5090 || 10iter: 23.7535 sec.\n",
            "イテレーション 3760 || Loss: 2.5429 || 10iter: 23.5782 sec.\n",
            "イテレーション 3770 || Loss: 2.5046 || 10iter: 23.7198 sec.\n",
            "イテレーション 3780 || Loss: 2.6123 || 10iter: 23.7464 sec.\n",
            "イテレーション 3790 || Loss: 2.5301 || 10iter: 23.6870 sec.\n",
            "イテレーション 3800 || Loss: 2.4671 || 10iter: 23.7267 sec.\n",
            "イテレーション 3810 || Loss: 2.5310 || 10iter: 23.7374 sec.\n",
            "イテレーション 3820 || Loss: 2.5412 || 10iter: 23.8707 sec.\n",
            "イテレーション 3830 || Loss: 2.5574 || 10iter: 23.7798 sec.\n",
            "イテレーション 3840 || Loss: 2.5346 || 10iter: 23.7111 sec.\n",
            "イテレーション 3850 || Loss: 2.4471 || 10iter: 23.7517 sec.\n",
            "イテレーション 3860 || Loss: 2.5799 || 10iter: 23.7292 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 27 || Epoch_TRAIN_Loss:363.1333 ||Epoch_VAL_Loss:385.9857\n",
            "timer:  399.4014 sec.\n",
            "-------------\n",
            "Epoch 28/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3870 || Loss: 2.4960 || 10iter: 21.0828 sec.\n",
            "イテレーション 3880 || Loss: 2.4906 || 10iter: 23.7996 sec.\n",
            "イテレーション 3890 || Loss: 2.6464 || 10iter: 23.6285 sec.\n",
            "イテレーション 3900 || Loss: 2.4918 || 10iter: 23.6349 sec.\n",
            "イテレーション 3910 || Loss: 2.5131 || 10iter: 23.6722 sec.\n",
            "イテレーション 3920 || Loss: 2.4906 || 10iter: 23.7025 sec.\n",
            "イテレーション 3930 || Loss: 2.4591 || 10iter: 23.8969 sec.\n",
            "イテレーション 3940 || Loss: 2.5005 || 10iter: 23.6601 sec.\n",
            "イテレーション 3950 || Loss: 2.6131 || 10iter: 23.8429 sec.\n",
            "イテレーション 3960 || Loss: 2.5199 || 10iter: 23.6502 sec.\n",
            "イテレーション 3970 || Loss: 2.5111 || 10iter: 23.7662 sec.\n",
            "イテレーション 3980 || Loss: 2.4881 || 10iter: 23.7217 sec.\n",
            "イテレーション 3990 || Loss: 2.5044 || 10iter: 23.7265 sec.\n",
            "イテレーション 4000 || Loss: 2.6047 || 10iter: 23.6716 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 28 || Epoch_TRAIN_Loss:361.4102 ||Epoch_VAL_Loss:387.0386\n",
            "timer:  398.9310 sec.\n",
            "-------------\n",
            "Epoch 29/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4010 || Loss: 2.5479 || 10iter: 13.7356 sec.\n",
            "イテレーション 4020 || Loss: 2.5596 || 10iter: 23.6025 sec.\n",
            "イテレーション 4030 || Loss: 2.5482 || 10iter: 23.7118 sec.\n",
            "イテレーション 4040 || Loss: 2.6214 || 10iter: 23.9788 sec.\n",
            "イテレーション 4050 || Loss: 2.7014 || 10iter: 23.6990 sec.\n",
            "イテレーション 4060 || Loss: 2.5445 || 10iter: 23.7439 sec.\n",
            "イテレーション 4070 || Loss: 2.5113 || 10iter: 23.7671 sec.\n",
            "イテレーション 4080 || Loss: 2.5281 || 10iter: 23.8343 sec.\n",
            "イテレーション 4090 || Loss: 2.5303 || 10iter: 23.7829 sec.\n",
            "イテレーション 4100 || Loss: 2.5135 || 10iter: 23.6862 sec.\n",
            "イテレーション 4110 || Loss: 2.5481 || 10iter: 23.6818 sec.\n",
            "イテレーション 4120 || Loss: 2.5578 || 10iter: 23.6958 sec.\n",
            "イテレーション 4130 || Loss: 2.4904 || 10iter: 23.6763 sec.\n",
            "イテレーション 4140 || Loss: 2.6321 || 10iter: 23.6553 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 29 || Epoch_TRAIN_Loss:362.2406 ||Epoch_VAL_Loss:389.2555\n",
            "timer:  399.5283 sec.\n",
            "-------------\n",
            "Epoch 30/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4150 || Loss: 2.4988 || 10iter: 6.1994 sec.\n",
            "イテレーション 4160 || Loss: 2.5928 || 10iter: 23.7167 sec.\n",
            "イテレーション 4170 || Loss: 2.4834 || 10iter: 23.6558 sec.\n",
            "イテレーション 4180 || Loss: 2.4493 || 10iter: 23.7291 sec.\n",
            "イテレーション 4190 || Loss: 2.6921 || 10iter: 23.8296 sec.\n",
            "イテレーション 4200 || Loss: 2.5353 || 10iter: 23.6567 sec.\n",
            "イテレーション 4210 || Loss: 2.5368 || 10iter: 23.6788 sec.\n",
            "イテレーション 4220 || Loss: 2.5337 || 10iter: 23.6822 sec.\n",
            "イテレーション 4230 || Loss: 2.4282 || 10iter: 23.6682 sec.\n",
            "イテレーション 4240 || Loss: 2.5126 || 10iter: 23.7799 sec.\n",
            "イテレーション 4250 || Loss: 2.4797 || 10iter: 23.7230 sec.\n",
            "イテレーション 4260 || Loss: 2.5109 || 10iter: 23.6999 sec.\n",
            "イテレーション 4270 || Loss: 2.5058 || 10iter: 23.8647 sec.\n",
            "イテレーション 4280 || Loss: 2.5090 || 10iter: 23.7928 sec.\n",
            "イテレーション 4290 || Loss: 2.4949 || 10iter: 23.5301 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 30 || Epoch_TRAIN_Loss:360.6289 ||Epoch_VAL_Loss:390.3996\n",
            "timer:  399.3341 sec.\n",
            "-------------\n",
            "Epoch 31/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4300 || Loss: 2.5175 || 10iter: 23.8228 sec.\n",
            "イテレーション 4310 || Loss: 2.4740 || 10iter: 23.6683 sec.\n",
            "イテレーション 4320 || Loss: 2.5026 || 10iter: 23.7165 sec.\n",
            "イテレーション 4330 || Loss: 2.5279 || 10iter: 23.6483 sec.\n",
            "イテレーション 4340 || Loss: 2.4950 || 10iter: 23.7086 sec.\n",
            "イテレーション 4350 || Loss: 2.4808 || 10iter: 23.5679 sec.\n",
            "イテレーション 4360 || Loss: 2.4714 || 10iter: 23.7475 sec.\n",
            "イテレーション 4370 || Loss: 2.5165 || 10iter: 23.6906 sec.\n",
            "イテレーション 4380 || Loss: 2.4782 || 10iter: 23.7913 sec.\n",
            "イテレーション 4390 || Loss: 2.5171 || 10iter: 23.7829 sec.\n",
            "イテレーション 4400 || Loss: 2.5676 || 10iter: 23.7446 sec.\n",
            "イテレーション 4410 || Loss: 2.4804 || 10iter: 23.6930 sec.\n",
            "イテレーション 4420 || Loss: 2.4847 || 10iter: 23.6898 sec.\n",
            "イテレーション 4430 || Loss: 2.5682 || 10iter: 23.6298 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 31 || Epoch_TRAIN_Loss:360.6316 ||Epoch_VAL_Loss:388.4918\n",
            "timer:  398.7501 sec.\n",
            "-------------\n",
            "Epoch 32/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4440 || Loss: 2.6735 || 10iter: 16.1646 sec.\n",
            "イテレーション 4450 || Loss: 2.4451 || 10iter: 23.5477 sec.\n",
            "イテレーション 4460 || Loss: 2.5091 || 10iter: 23.6254 sec.\n",
            "イテレーション 4470 || Loss: 2.5900 || 10iter: 23.6012 sec.\n",
            "イテレーション 4480 || Loss: 2.5052 || 10iter: 23.6809 sec.\n",
            "イテレーション 4490 || Loss: 2.4668 || 10iter: 23.7264 sec.\n",
            "イテレーション 4500 || Loss: 2.4777 || 10iter: 23.7187 sec.\n",
            "イテレーション 4510 || Loss: 2.4702 || 10iter: 23.4533 sec.\n",
            "イテレーション 4520 || Loss: 2.4829 || 10iter: 23.6336 sec.\n",
            "イテレーション 4530 || Loss: 2.4570 || 10iter: 23.5112 sec.\n",
            "イテレーション 4540 || Loss: 2.4732 || 10iter: 23.5768 sec.\n",
            "イテレーション 4550 || Loss: 2.5156 || 10iter: 23.5812 sec.\n",
            "イテレーション 4560 || Loss: 2.5287 || 10iter: 23.5503 sec.\n",
            "イテレーション 4570 || Loss: 2.5643 || 10iter: 23.5521 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 32 || Epoch_TRAIN_Loss:358.3045 ||Epoch_VAL_Loss:389.4282\n",
            "timer:  396.8782 sec.\n",
            "-------------\n",
            "Epoch 33/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4580 || Loss: 2.5248 || 10iter: 8.6607 sec.\n",
            "イテレーション 4590 || Loss: 2.4495 || 10iter: 23.6412 sec.\n",
            "イテレーション 4600 || Loss: 2.4745 || 10iter: 23.7172 sec.\n",
            "イテレーション 4610 || Loss: 2.4477 || 10iter: 23.5379 sec.\n",
            "イテレーション 4620 || Loss: 2.5445 || 10iter: 23.6013 sec.\n",
            "イテレーション 4630 || Loss: 2.8191 || 10iter: 23.7154 sec.\n",
            "イテレーション 4640 || Loss: 2.4737 || 10iter: 23.6234 sec.\n",
            "イテレーション 4650 || Loss: 2.5200 || 10iter: 23.6088 sec.\n",
            "イテレーション 4660 || Loss: 2.5392 || 10iter: 23.5997 sec.\n",
            "イテレーション 4670 || Loss: 2.4903 || 10iter: 23.5253 sec.\n",
            "イテレーション 4680 || Loss: 2.4687 || 10iter: 23.7307 sec.\n",
            "イテレーション 4690 || Loss: 2.4407 || 10iter: 23.5251 sec.\n",
            "イテレーション 4700 || Loss: 2.4737 || 10iter: 23.7074 sec.\n",
            "イテレーション 4710 || Loss: 2.5367 || 10iter: 23.5308 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 33 || Epoch_TRAIN_Loss:358.2651 ||Epoch_VAL_Loss:390.2100\n",
            "timer:  397.4985 sec.\n",
            "-------------\n",
            "Epoch 34/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4720 || Loss: 2.5079 || 10iter: 1.2093 sec.\n",
            "イテレーション 4730 || Loss: 2.4667 || 10iter: 23.6097 sec.\n",
            "イテレーション 4740 || Loss: 2.5361 || 10iter: 23.5739 sec.\n",
            "イテレーション 4750 || Loss: 2.4866 || 10iter: 23.5260 sec.\n",
            "イテレーション 4760 || Loss: 2.4821 || 10iter: 23.6591 sec.\n",
            "イテレーション 4770 || Loss: 2.4765 || 10iter: 23.5331 sec.\n",
            "イテレーション 4780 || Loss: 2.5004 || 10iter: 23.5327 sec.\n",
            "イテレーション 4790 || Loss: 2.4929 || 10iter: 23.5322 sec.\n",
            "イテレーション 4800 || Loss: 2.5359 || 10iter: 23.6144 sec.\n",
            "イテレーション 4810 || Loss: 2.4716 || 10iter: 23.6299 sec.\n",
            "イテレーション 4820 || Loss: 2.4330 || 10iter: 23.5534 sec.\n",
            "イテレーション 4830 || Loss: 2.5089 || 10iter: 23.7619 sec.\n",
            "イテレーション 4840 || Loss: 2.4993 || 10iter: 23.5162 sec.\n",
            "イテレーション 4850 || Loss: 2.4682 || 10iter: 23.4955 sec.\n",
            "イテレーション 4860 || Loss: 2.6731 || 10iter: 23.5686 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 34 || Epoch_TRAIN_Loss:357.1831 ||Epoch_VAL_Loss:389.2329\n",
            "timer:  396.4870 sec.\n",
            "-------------\n",
            "Epoch 35/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4870 || Loss: 2.5221 || 10iter: 18.5557 sec.\n",
            "イテレーション 4880 || Loss: 2.4498 || 10iter: 23.5361 sec.\n",
            "イテレーション 4890 || Loss: 2.4803 || 10iter: 23.4427 sec.\n",
            "イテレーション 4900 || Loss: 2.4161 || 10iter: 23.5717 sec.\n",
            "イテレーション 4910 || Loss: 2.5021 || 10iter: 23.4779 sec.\n",
            "イテレーション 4920 || Loss: 2.5268 || 10iter: 23.4676 sec.\n",
            "イテレーション 4930 || Loss: 2.4988 || 10iter: 23.4721 sec.\n",
            "イテレーション 4940 || Loss: 2.4986 || 10iter: 23.6844 sec.\n",
            "イテレーション 4950 || Loss: 2.5069 || 10iter: 23.4168 sec.\n",
            "イテレーション 4960 || Loss: 2.5400 || 10iter: 23.5667 sec.\n",
            "イテレーション 4970 || Loss: 2.4825 || 10iter: 23.5728 sec.\n",
            "イテレーション 4980 || Loss: 2.5453 || 10iter: 23.6084 sec.\n",
            "イテレーション 4990 || Loss: 2.4875 || 10iter: 23.6469 sec.\n",
            "イテレーション 5000 || Loss: 2.4955 || 10iter: 23.5327 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 35 || Epoch_TRAIN_Loss:357.4808 ||Epoch_VAL_Loss:393.4828\n",
            "timer:  395.8365 sec.\n",
            "-------------\n",
            "Epoch 36/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5010 || Loss: 2.4696 || 10iter: 11.1014 sec.\n",
            "イテレーション 5020 || Loss: 2.4992 || 10iter: 23.6022 sec.\n",
            "イテレーション 5030 || Loss: 2.4423 || 10iter: 23.4720 sec.\n",
            "イテレーション 5040 || Loss: 2.4556 || 10iter: 23.5499 sec.\n",
            "イテレーション 5050 || Loss: 2.4546 || 10iter: 23.6253 sec.\n",
            "イテレーション 5060 || Loss: 2.4089 || 10iter: 23.5000 sec.\n",
            "イテレーション 5070 || Loss: 2.5377 || 10iter: 23.6240 sec.\n",
            "イテレーション 5080 || Loss: 2.4872 || 10iter: 23.5493 sec.\n",
            "イテレーション 5090 || Loss: 2.4891 || 10iter: 23.4664 sec.\n",
            "イテレーション 5100 || Loss: 2.4829 || 10iter: 23.4411 sec.\n",
            "イテレーション 5110 || Loss: 2.4552 || 10iter: 23.3657 sec.\n",
            "イテレーション 5120 || Loss: 2.4482 || 10iter: 23.5279 sec.\n",
            "イテレーション 5130 || Loss: 2.5325 || 10iter: 23.4324 sec.\n",
            "イテレーション 5140 || Loss: 2.4909 || 10iter: 23.4346 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 36 || Epoch_TRAIN_Loss:356.5500 ||Epoch_VAL_Loss:393.3833\n",
            "timer:  395.5246 sec.\n",
            "-------------\n",
            "Epoch 37/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5150 || Loss: 2.4141 || 10iter: 3.6821 sec.\n",
            "イテレーション 5160 || Loss: 2.4921 || 10iter: 23.4355 sec.\n",
            "イテレーション 5170 || Loss: 2.4334 || 10iter: 23.4769 sec.\n",
            "イテレーション 5180 || Loss: 2.4499 || 10iter: 23.5522 sec.\n",
            "イテレーション 5190 || Loss: 2.4909 || 10iter: 23.4689 sec.\n",
            "イテレーション 5200 || Loss: 2.5255 || 10iter: 23.4629 sec.\n",
            "イテレーション 5210 || Loss: 2.4978 || 10iter: 23.4460 sec.\n",
            "イテレーション 5220 || Loss: 2.4714 || 10iter: 23.4315 sec.\n",
            "イテレーション 5230 || Loss: 2.4077 || 10iter: 23.4825 sec.\n",
            "イテレーション 5240 || Loss: 2.4718 || 10iter: 23.3795 sec.\n",
            "イテレーション 5250 || Loss: 2.4584 || 10iter: 23.4842 sec.\n",
            "イテレーション 5260 || Loss: 2.5251 || 10iter: 23.4662 sec.\n",
            "イテレーション 5270 || Loss: 2.5149 || 10iter: 23.5551 sec.\n",
            "イテレーション 5280 || Loss: 2.5030 || 10iter: 23.5944 sec.\n",
            "イテレーション 5290 || Loss: 2.4842 || 10iter: 23.4286 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 37 || Epoch_TRAIN_Loss:357.0162 ||Epoch_VAL_Loss:395.2043\n",
            "timer:  395.1768 sec.\n",
            "-------------\n",
            "Epoch 38/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5300 || Loss: 2.5251 || 10iter: 21.0166 sec.\n",
            "イテレーション 5310 || Loss: 2.4570 || 10iter: 23.4288 sec.\n",
            "イテレーション 5320 || Loss: 2.4801 || 10iter: 23.5238 sec.\n",
            "イテレーション 5330 || Loss: 2.4283 || 10iter: 23.4267 sec.\n",
            "イテレーション 5340 || Loss: 2.5037 || 10iter: 23.5077 sec.\n",
            "イテレーション 5350 || Loss: 2.4159 || 10iter: 23.4385 sec.\n",
            "イテレーション 5360 || Loss: 2.4765 || 10iter: 23.5411 sec.\n",
            "イテレーション 5370 || Loss: 2.4721 || 10iter: 23.4568 sec.\n",
            "イテレーション 5380 || Loss: 2.4579 || 10iter: 23.5299 sec.\n",
            "イテレーション 5390 || Loss: 2.5231 || 10iter: 23.6946 sec.\n",
            "イテレーション 5400 || Loss: 2.5801 || 10iter: 23.4439 sec.\n",
            "イテレーション 5410 || Loss: 2.4936 || 10iter: 23.5941 sec.\n",
            "イテレーション 5420 || Loss: 2.4224 || 10iter: 23.5117 sec.\n",
            "イテレーション 5430 || Loss: 2.4786 || 10iter: 23.5104 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 38 || Epoch_TRAIN_Loss:355.9354 ||Epoch_VAL_Loss:394.3276\n",
            "timer:  395.3646 sec.\n",
            "-------------\n",
            "Epoch 39/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5440 || Loss: 2.4469 || 10iter: 13.5781 sec.\n",
            "イテレーション 5450 || Loss: 2.5105 || 10iter: 23.5381 sec.\n",
            "イテレーション 5460 || Loss: 2.6365 || 10iter: 23.4726 sec.\n",
            "イテレーション 5470 || Loss: 2.4679 || 10iter: 23.4426 sec.\n",
            "イテレーション 5480 || Loss: 2.4965 || 10iter: 23.4983 sec.\n",
            "イテレーション 5490 || Loss: 2.5021 || 10iter: 23.4863 sec.\n",
            "イテレーション 5500 || Loss: 2.5142 || 10iter: 23.6001 sec.\n",
            "イテレーション 5510 || Loss: 2.6283 || 10iter: 23.4545 sec.\n",
            "イテレーション 5520 || Loss: 2.5878 || 10iter: 23.4705 sec.\n",
            "イテレーション 5530 || Loss: 2.5432 || 10iter: 23.3821 sec.\n",
            "イテレーション 5540 || Loss: 2.5979 || 10iter: 23.4387 sec.\n",
            "イテレーション 5550 || Loss: 2.4808 || 10iter: 23.4484 sec.\n",
            "イテレーション 5560 || Loss: 2.5105 || 10iter: 23.5268 sec.\n",
            "イテレーション 5570 || Loss: 2.5679 || 10iter: 23.4215 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 39 || Epoch_TRAIN_Loss:356.3289 ||Epoch_VAL_Loss:395.8077\n",
            "timer:  394.7512 sec.\n",
            "-------------\n",
            "Epoch 40/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5580 || Loss: 2.5238 || 10iter: 6.1694 sec.\n",
            "イテレーション 5590 || Loss: 2.4254 || 10iter: 23.4243 sec.\n",
            "イテレーション 5600 || Loss: 2.4012 || 10iter: 23.5149 sec.\n",
            "イテレーション 5610 || Loss: 2.4306 || 10iter: 23.6073 sec.\n",
            "イテレーション 5620 || Loss: 2.3938 || 10iter: 23.4318 sec.\n",
            "イテレーション 5630 || Loss: 2.4693 || 10iter: 23.4680 sec.\n",
            "イテレーション 5640 || Loss: 2.3875 || 10iter: 23.4685 sec.\n",
            "イテレーション 5650 || Loss: 2.4661 || 10iter: 23.5819 sec.\n",
            "イテレーション 5660 || Loss: 2.4472 || 10iter: 23.4095 sec.\n",
            "イテレーション 5670 || Loss: 2.4884 || 10iter: 23.4401 sec.\n",
            "イテレーション 5680 || Loss: 2.4286 || 10iter: 23.3874 sec.\n",
            "イテレーション 5690 || Loss: 2.3766 || 10iter: 23.5160 sec.\n",
            "イテレーション 5700 || Loss: 2.5214 || 10iter: 23.5066 sec.\n",
            "イテレーション 5710 || Loss: 2.5467 || 10iter: 23.5322 sec.\n",
            "イテレーション 5720 || Loss: 2.4667 || 10iter: 23.2710 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 40 || Epoch_TRAIN_Loss:354.8682 ||Epoch_VAL_Loss:395.1183\n",
            "timer:  395.4621 sec.\n",
            "-------------\n",
            "Epoch 41/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5730 || Loss: 2.4642 || 10iter: 23.6635 sec.\n",
            "イテレーション 5740 || Loss: 2.7634 || 10iter: 23.4240 sec.\n",
            "イテレーション 5750 || Loss: 2.6045 || 10iter: 23.5593 sec.\n",
            "イテレーション 5760 || Loss: 2.4444 || 10iter: 23.4779 sec.\n",
            "イテレーション 5770 || Loss: 2.4304 || 10iter: 23.5134 sec.\n",
            "イテレーション 5780 || Loss: 2.5476 || 10iter: 23.4917 sec.\n",
            "イテレーション 5790 || Loss: 2.4303 || 10iter: 23.5844 sec.\n",
            "イテレーション 5800 || Loss: 2.4798 || 10iter: 23.5220 sec.\n",
            "イテレーション 5810 || Loss: 2.4580 || 10iter: 23.4324 sec.\n",
            "イテレーション 5820 || Loss: 2.4553 || 10iter: 23.4390 sec.\n",
            "イテレーション 5830 || Loss: 2.4726 || 10iter: 23.4854 sec.\n",
            "イテレーション 5840 || Loss: 2.5856 || 10iter: 23.6014 sec.\n",
            "イテレーション 5850 || Loss: 2.4578 || 10iter: 23.5970 sec.\n",
            "イテレーション 5860 || Loss: 2.5122 || 10iter: 23.3870 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 41 || Epoch_TRAIN_Loss:355.1675 ||Epoch_VAL_Loss:395.7178\n",
            "timer:  395.4902 sec.\n",
            "-------------\n",
            "Epoch 42/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5870 || Loss: 2.4136 || 10iter: 16.0639 sec.\n",
            "イテレーション 5880 || Loss: 2.4556 || 10iter: 23.6239 sec.\n",
            "イテレーション 5890 || Loss: 2.5161 || 10iter: 23.4972 sec.\n",
            "イテレーション 5900 || Loss: 2.3985 || 10iter: 23.5025 sec.\n",
            "イテレーション 5910 || Loss: 2.4387 || 10iter: 23.5985 sec.\n",
            "イテレーション 5920 || Loss: 2.4222 || 10iter: 23.5087 sec.\n",
            "イテレーション 5930 || Loss: 2.6722 || 10iter: 23.4923 sec.\n",
            "イテレーション 5940 || Loss: 2.4164 || 10iter: 23.6026 sec.\n",
            "イテレーション 5950 || Loss: 2.4645 || 10iter: 23.6564 sec.\n",
            "イテレーション 5960 || Loss: 2.4373 || 10iter: 23.6742 sec.\n",
            "イテレーション 5970 || Loss: 2.4437 || 10iter: 23.4693 sec.\n",
            "イテレーション 5980 || Loss: 2.5106 || 10iter: 23.5526 sec.\n",
            "イテレーション 5990 || Loss: 2.4422 || 10iter: 23.5541 sec.\n",
            "イテレーション 6000 || Loss: 2.4598 || 10iter: 23.5693 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 42 || Epoch_TRAIN_Loss:353.9422 ||Epoch_VAL_Loss:397.0605\n",
            "timer:  396.4705 sec.\n",
            "-------------\n",
            "Epoch 43/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 6010 || Loss: 2.3992 || 10iter: 8.6827 sec.\n",
            "イテレーション 6020 || Loss: 2.7543 || 10iter: 23.5077 sec.\n",
            "イテレーション 6030 || Loss: 2.5268 || 10iter: 23.5318 sec.\n",
            "イテレーション 6040 || Loss: 2.5023 || 10iter: 23.5465 sec.\n",
            "イテレーション 6050 || Loss: 2.4423 || 10iter: 23.5031 sec.\n",
            "イテレーション 6060 || Loss: 2.4378 || 10iter: 23.7668 sec.\n",
            "イテレーション 6070 || Loss: 2.4804 || 10iter: 23.5406 sec.\n",
            "イテレーション 6080 || Loss: 2.4469 || 10iter: 23.5435 sec.\n",
            "イテレーション 6090 || Loss: 2.5780 || 10iter: 23.5387 sec.\n",
            "イテレーション 6100 || Loss: 2.4453 || 10iter: 23.6212 sec.\n",
            "イテレーション 6110 || Loss: 2.6175 || 10iter: 23.6563 sec.\n",
            "イテレーション 6120 || Loss: 2.5069 || 10iter: 23.5811 sec.\n",
            "イテレーション 6130 || Loss: 2.4197 || 10iter: 23.5987 sec.\n",
            "イテレーション 6140 || Loss: 2.4061 || 10iter: 23.5953 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 43 || Epoch_TRAIN_Loss:354.5141 ||Epoch_VAL_Loss:399.1635\n",
            "timer:  397.0179 sec.\n",
            "-------------\n",
            "Epoch 44/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 6150 || Loss: 2.3922 || 10iter: 1.2097 sec.\n",
            "イテレーション 6160 || Loss: 2.4496 || 10iter: 23.7826 sec.\n",
            "イテレーション 6170 || Loss: 2.4794 || 10iter: 23.7787 sec.\n",
            "イテレーション 6180 || Loss: 2.5591 || 10iter: 23.6630 sec.\n",
            "イテレーション 6190 || Loss: 2.5055 || 10iter: 23.5022 sec.\n",
            "イテレーション 6200 || Loss: 2.5319 || 10iter: 23.7258 sec.\n",
            "イテレーション 6210 || Loss: 2.4081 || 10iter: 23.6917 sec.\n",
            "イテレーション 6220 || Loss: 2.4784 || 10iter: 23.5761 sec.\n",
            "イテレーション 6230 || Loss: 2.4953 || 10iter: 23.4691 sec.\n",
            "イテレーション 6240 || Loss: 2.4184 || 10iter: 23.6685 sec.\n",
            "イテレーション 6250 || Loss: 2.4716 || 10iter: 23.5829 sec.\n",
            "イテレーション 6260 || Loss: 2.4708 || 10iter: 23.6804 sec.\n",
            "イテレーション 6270 || Loss: 2.4583 || 10iter: 23.5934 sec.\n",
            "イテレーション 6280 || Loss: 2.5356 || 10iter: 23.6087 sec.\n",
            "イテレーション 6290 || Loss: 2.4090 || 10iter: 23.8877 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 44 || Epoch_TRAIN_Loss:354.4128 ||Epoch_VAL_Loss:398.6691\n",
            "timer:  397.8830 sec.\n",
            "-------------\n",
            "Epoch 45/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 6300 || Loss: 2.5166 || 10iter: 18.6270 sec.\n",
            "イテレーション 6310 || Loss: 2.4671 || 10iter: 23.6590 sec.\n",
            "イテレーション 6320 || Loss: 2.3974 || 10iter: 23.5577 sec.\n",
            "イテレーション 6330 || Loss: 2.4039 || 10iter: 23.7418 sec.\n",
            "イテレーション 6340 || Loss: 2.4584 || 10iter: 23.5255 sec.\n",
            "イテレーション 6350 || Loss: 2.4358 || 10iter: 23.5756 sec.\n",
            "イテレーション 6360 || Loss: 2.4184 || 10iter: 23.4724 sec.\n",
            "イテレーション 6370 || Loss: 2.5568 || 10iter: 23.6394 sec.\n",
            "イテレーション 6380 || Loss: 2.4749 || 10iter: 23.6853 sec.\n",
            "イテレーション 6390 || Loss: 2.4351 || 10iter: 23.5050 sec.\n",
            "イテレーション 6400 || Loss: 2.4515 || 10iter: 23.7990 sec.\n",
            "イテレーション 6410 || Loss: 2.4185 || 10iter: 23.5540 sec.\n",
            "イテレーション 6420 || Loss: 2.4006 || 10iter: 23.5977 sec.\n",
            "イテレーション 6430 || Loss: 2.4485 || 10iter: 23.4027 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 45 || Epoch_TRAIN_Loss:352.4106 ||Epoch_VAL_Loss:399.4582\n",
            "timer:  396.6599 sec.\n",
            "-------------\n",
            "Epoch 46/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 6440 || Loss: 2.4810 || 10iter: 11.1134 sec.\n",
            "イテレーション 6450 || Loss: 2.5405 || 10iter: 23.4674 sec.\n",
            "イテレーション 6460 || Loss: 2.4428 || 10iter: 23.5129 sec.\n",
            "イテレーション 6470 || Loss: 2.4766 || 10iter: 23.5470 sec.\n",
            "イテレーション 6480 || Loss: 2.4733 || 10iter: 23.5219 sec.\n",
            "イテレーション 6490 || Loss: 2.4794 || 10iter: 23.5198 sec.\n",
            "イテレーション 6500 || Loss: 2.4140 || 10iter: 23.4972 sec.\n",
            "イテレーション 6510 || Loss: 2.4920 || 10iter: 23.8202 sec.\n",
            "イテレーション 6520 || Loss: 2.4753 || 10iter: 23.5954 sec.\n",
            "イテレーション 6530 || Loss: 2.4714 || 10iter: 23.5772 sec.\n",
            "イテレーション 6540 || Loss: 2.4417 || 10iter: 23.5969 sec.\n",
            "イテレーション 6550 || Loss: 2.4920 || 10iter: 23.6478 sec.\n",
            "イテレーション 6560 || Loss: 2.8845 || 10iter: 23.6364 sec.\n",
            "イテレーション 6570 || Loss: 2.4993 || 10iter: 23.6998 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 46 || Epoch_TRAIN_Loss:353.2713 ||Epoch_VAL_Loss:397.7974\n",
            "timer:  396.5707 sec.\n",
            "-------------\n",
            "Epoch 47/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 6580 || Loss: 2.4398 || 10iter: 3.7056 sec.\n",
            "イテレーション 6590 || Loss: 2.5011 || 10iter: 23.5516 sec.\n",
            "イテレーション 6600 || Loss: 2.5106 || 10iter: 23.5367 sec.\n",
            "イテレーション 6610 || Loss: 2.4371 || 10iter: 23.7349 sec.\n",
            "イテレーション 6620 || Loss: 2.4525 || 10iter: 23.5584 sec.\n",
            "イテレーション 6630 || Loss: 2.5115 || 10iter: 23.5827 sec.\n",
            "イテレーション 6640 || Loss: 2.4794 || 10iter: 23.5759 sec.\n",
            "イテレーション 6650 || Loss: 2.4774 || 10iter: 23.4691 sec.\n",
            "イテレーション 6660 || Loss: 2.4402 || 10iter: 23.5253 sec.\n",
            "イテレーション 6670 || Loss: 2.6083 || 10iter: 23.5066 sec.\n",
            "イテレーション 6680 || Loss: 2.4079 || 10iter: 23.7022 sec.\n",
            "イテレーション 6690 || Loss: 2.4900 || 10iter: 23.5690 sec.\n",
            "イテレーション 6700 || Loss: 2.5079 || 10iter: 23.6172 sec.\n",
            "イテレーション 6710 || Loss: 2.3763 || 10iter: 23.5504 sec.\n",
            "イテレーション 6720 || Loss: 2.5619 || 10iter: 23.5535 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 47 || Epoch_TRAIN_Loss:353.7980 ||Epoch_VAL_Loss:401.4980\n",
            "timer:  397.0877 sec.\n",
            "-------------\n",
            "Epoch 48/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 6730 || Loss: 2.4343 || 10iter: 21.0794 sec.\n",
            "イテレーション 6740 || Loss: 2.4588 || 10iter: 23.5105 sec.\n",
            "イテレーション 6750 || Loss: 2.4528 || 10iter: 23.6429 sec.\n",
            "イテレーション 6760 || Loss: 2.5000 || 10iter: 23.5159 sec.\n",
            "イテレーション 6770 || Loss: 2.4008 || 10iter: 23.6346 sec.\n",
            "イテレーション 6780 || Loss: 2.4686 || 10iter: 23.5698 sec.\n",
            "イテレーション 6790 || Loss: 2.4621 || 10iter: 23.5492 sec.\n",
            "イテレーション 6800 || Loss: 2.4417 || 10iter: 23.5760 sec.\n",
            "イテレーション 6810 || Loss: 2.5508 || 10iter: 23.5304 sec.\n",
            "イテレーション 6820 || Loss: 2.4551 || 10iter: 23.6266 sec.\n",
            "イテレーション 6830 || Loss: 2.4518 || 10iter: 23.5951 sec.\n",
            "イテレーション 6840 || Loss: 2.5373 || 10iter: 23.7542 sec.\n",
            "イテレーション 6850 || Loss: 2.4655 || 10iter: 23.7871 sec.\n",
            "イテレーション 6860 || Loss: 2.4477 || 10iter: 23.4139 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 48 || Epoch_TRAIN_Loss:352.3210 ||Epoch_VAL_Loss:399.2234\n",
            "timer:  396.7542 sec.\n",
            "-------------\n",
            "Epoch 49/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 6870 || Loss: 2.4804 || 10iter: 13.6071 sec.\n",
            "イテレーション 6880 || Loss: 2.5269 || 10iter: 23.5525 sec.\n",
            "イテレーション 6890 || Loss: 2.5168 || 10iter: 23.4894 sec.\n",
            "イテレーション 6900 || Loss: 2.4262 || 10iter: 23.5594 sec.\n",
            "イテレーション 6910 || Loss: 2.6012 || 10iter: 23.4436 sec.\n",
            "イテレーション 6920 || Loss: 2.5571 || 10iter: 23.5254 sec.\n",
            "イテレーション 6930 || Loss: 2.4122 || 10iter: 23.5412 sec.\n",
            "イテレーション 6940 || Loss: 2.4776 || 10iter: 23.5431 sec.\n",
            "イテレーション 6950 || Loss: 2.4327 || 10iter: 23.6073 sec.\n",
            "イテレーション 6960 || Loss: 2.4664 || 10iter: 23.8509 sec.\n",
            "イテレーション 6970 || Loss: 2.4272 || 10iter: 23.6161 sec.\n",
            "イテレーション 6980 || Loss: 2.4073 || 10iter: 23.5454 sec.\n",
            "イテレーション 6990 || Loss: 2.4577 || 10iter: 23.5175 sec.\n",
            "イテレーション 7000 || Loss: 2.4943 || 10iter: 23.4607 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 49 || Epoch_TRAIN_Loss:352.5001 ||Epoch_VAL_Loss:400.3320\n",
            "timer:  396.3314 sec.\n",
            "-------------\n",
            "Epoch 50/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 7010 || Loss: 2.4619 || 10iter: 6.1585 sec.\n",
            "イテレーション 7020 || Loss: 2.4175 || 10iter: 23.6050 sec.\n",
            "イテレーション 7030 || Loss: 2.5744 || 10iter: 23.5353 sec.\n",
            "イテレーション 7040 || Loss: 2.4291 || 10iter: 23.5810 sec.\n",
            "イテレーション 7050 || Loss: 2.3911 || 10iter: 23.5403 sec.\n",
            "イテレーション 7060 || Loss: 2.4666 || 10iter: 23.7538 sec.\n",
            "イテレーション 7070 || Loss: 2.4280 || 10iter: 23.6827 sec.\n",
            "イテレーション 7080 || Loss: 2.4893 || 10iter: 23.6108 sec.\n",
            "イテレーション 7090 || Loss: 2.4791 || 10iter: 23.6820 sec.\n",
            "イテレーション 7100 || Loss: 2.5363 || 10iter: 23.7486 sec.\n",
            "イテレーション 7110 || Loss: 2.4270 || 10iter: 23.6647 sec.\n",
            "イテレーション 7120 || Loss: 2.6829 || 10iter: 23.7376 sec.\n",
            "イテレーション 7130 || Loss: 2.4441 || 10iter: 23.5587 sec.\n",
            "イテレーション 7140 || Loss: 2.4442 || 10iter: 23.6807 sec.\n",
            "イテレーション 7150 || Loss: 2.4511 || 10iter: 23.4644 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 50 || Epoch_TRAIN_Loss:350.7876 ||Epoch_VAL_Loss:400.9789\n",
            "timer:  397.8689 sec.\n",
            "-------------\n",
            "Epoch 51/50\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 7160 || Loss: 2.4071 || 10iter: 23.7981 sec.\n",
            "イテレーション 7170 || Loss: 2.4232 || 10iter: 23.8862 sec.\n",
            "イテレーション 7180 || Loss: 2.4513 || 10iter: 23.6674 sec.\n",
            "イテレーション 7190 || Loss: 2.4691 || 10iter: 23.7977 sec.\n",
            "イテレーション 7200 || Loss: 2.4511 || 10iter: 23.7606 sec.\n",
            "イテレーション 7210 || Loss: 2.4160 || 10iter: 23.6923 sec.\n",
            "イテレーション 7220 || Loss: 2.4414 || 10iter: 23.5700 sec.\n",
            "イテレーション 7230 || Loss: 2.5274 || 10iter: 23.6089 sec.\n",
            "イテレーション 7240 || Loss: 2.4475 || 10iter: 23.6505 sec.\n",
            "イテレーション 7250 || Loss: 2.5823 || 10iter: 23.5829 sec.\n",
            "イテレーション 7260 || Loss: 2.4746 || 10iter: 23.5554 sec.\n",
            "イテレーション 7270 || Loss: 2.4402 || 10iter: 23.6440 sec.\n",
            "イテレーション 7280 || Loss: 2.4760 || 10iter: 23.5921 sec.\n",
            "イテレーション 7290 || Loss: 2.4658 || 10iter: 23.5644 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 51 || Epoch_TRAIN_Loss:351.7601 ||Epoch_VAL_Loss:402.0372\n",
            "timer:  398.0652 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfLmp2_6AWiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_output = pd.read_csv(path + 'log_output.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CweE42BYAWfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46b8b7ba-b817-4e48-f1c5-4231d8526ad7"
      },
      "source": [
        "log_output"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1249.158659</td>\n",
              "      <td>294.711149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>605.281198</td>\n",
              "      <td>311.634125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>515.715875</td>\n",
              "      <td>326.073635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>469.613664</td>\n",
              "      <td>338.999422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>444.073776</td>\n",
              "      <td>343.448923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>427.789749</td>\n",
              "      <td>350.530606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>415.578062</td>\n",
              "      <td>354.588650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>407.414358</td>\n",
              "      <td>356.979826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>400.440418</td>\n",
              "      <td>361.370153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>396.125629</td>\n",
              "      <td>361.123581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>391.512480</td>\n",
              "      <td>365.766411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>387.570566</td>\n",
              "      <td>364.228605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>383.083643</td>\n",
              "      <td>372.442955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>380.800639</td>\n",
              "      <td>371.087479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>379.350936</td>\n",
              "      <td>370.773327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>376.995951</td>\n",
              "      <td>372.213184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "      <td>373.970971</td>\n",
              "      <td>374.632071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>18</td>\n",
              "      <td>373.679919</td>\n",
              "      <td>374.887969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>19</td>\n",
              "      <td>370.949608</td>\n",
              "      <td>375.644917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>369.482023</td>\n",
              "      <td>379.641226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>21</td>\n",
              "      <td>370.197453</td>\n",
              "      <td>380.675052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>368.565024</td>\n",
              "      <td>381.563902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>23</td>\n",
              "      <td>366.058605</td>\n",
              "      <td>382.900351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>24</td>\n",
              "      <td>365.419808</td>\n",
              "      <td>382.327996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>363.321108</td>\n",
              "      <td>382.169757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>363.819919</td>\n",
              "      <td>385.835165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>27</td>\n",
              "      <td>363.133314</td>\n",
              "      <td>385.985726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>28</td>\n",
              "      <td>361.410237</td>\n",
              "      <td>387.038598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>29</td>\n",
              "      <td>362.240553</td>\n",
              "      <td>389.255490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>30</td>\n",
              "      <td>360.628949</td>\n",
              "      <td>390.399586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>31</td>\n",
              "      <td>360.631618</td>\n",
              "      <td>388.491758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>32</td>\n",
              "      <td>358.304458</td>\n",
              "      <td>389.428195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>358.265109</td>\n",
              "      <td>390.209966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>34</td>\n",
              "      <td>357.183116</td>\n",
              "      <td>389.232857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>35</td>\n",
              "      <td>357.480794</td>\n",
              "      <td>393.482849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>36</td>\n",
              "      <td>356.549959</td>\n",
              "      <td>393.383324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>37</td>\n",
              "      <td>357.016212</td>\n",
              "      <td>395.204275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>355.935376</td>\n",
              "      <td>394.327585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>39</td>\n",
              "      <td>356.328853</td>\n",
              "      <td>395.807737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>40</td>\n",
              "      <td>354.868159</td>\n",
              "      <td>395.118266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>40</td>\n",
              "      <td>41</td>\n",
              "      <td>355.167474</td>\n",
              "      <td>395.717762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41</td>\n",
              "      <td>42</td>\n",
              "      <td>353.942184</td>\n",
              "      <td>397.060537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>43</td>\n",
              "      <td>354.514065</td>\n",
              "      <td>399.163528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>44</td>\n",
              "      <td>354.412752</td>\n",
              "      <td>398.669116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>44</td>\n",
              "      <td>45</td>\n",
              "      <td>352.410634</td>\n",
              "      <td>399.458223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>45</td>\n",
              "      <td>46</td>\n",
              "      <td>353.271284</td>\n",
              "      <td>397.797400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>46</td>\n",
              "      <td>47</td>\n",
              "      <td>353.798015</td>\n",
              "      <td>401.497966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>48</td>\n",
              "      <td>352.321050</td>\n",
              "      <td>399.223367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>48</td>\n",
              "      <td>49</td>\n",
              "      <td>352.500059</td>\n",
              "      <td>400.331973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>49</td>\n",
              "      <td>50</td>\n",
              "      <td>350.787606</td>\n",
              "      <td>400.978913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>50</td>\n",
              "      <td>51</td>\n",
              "      <td>351.760088</td>\n",
              "      <td>402.037236</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0  epoch   train_loss    val_loss\n",
              "0            0      1  1249.158659  294.711149\n",
              "1            1      2   605.281198  311.634125\n",
              "2            2      3   515.715875  326.073635\n",
              "3            3      4   469.613664  338.999422\n",
              "4            4      5   444.073776  343.448923\n",
              "5            5      6   427.789749  350.530606\n",
              "6            6      7   415.578062  354.588650\n",
              "7            7      8   407.414358  356.979826\n",
              "8            8      9   400.440418  361.370153\n",
              "9            9     10   396.125629  361.123581\n",
              "10          10     11   391.512480  365.766411\n",
              "11          11     12   387.570566  364.228605\n",
              "12          12     13   383.083643  372.442955\n",
              "13          13     14   380.800639  371.087479\n",
              "14          14     15   379.350936  370.773327\n",
              "15          15     16   376.995951  372.213184\n",
              "16          16     17   373.970971  374.632071\n",
              "17          17     18   373.679919  374.887969\n",
              "18          18     19   370.949608  375.644917\n",
              "19          19     20   369.482023  379.641226\n",
              "20          20     21   370.197453  380.675052\n",
              "21          21     22   368.565024  381.563902\n",
              "22          22     23   366.058605  382.900351\n",
              "23          23     24   365.419808  382.327996\n",
              "24          24     25   363.321108  382.169757\n",
              "25          25     26   363.819919  385.835165\n",
              "26          26     27   363.133314  385.985726\n",
              "27          27     28   361.410237  387.038598\n",
              "28          28     29   362.240553  389.255490\n",
              "29          29     30   360.628949  390.399586\n",
              "30          30     31   360.631618  388.491758\n",
              "31          31     32   358.304458  389.428195\n",
              "32          32     33   358.265109  390.209966\n",
              "33          33     34   357.183116  389.232857\n",
              "34          34     35   357.480794  393.482849\n",
              "35          35     36   356.549959  393.383324\n",
              "36          36     37   357.016212  395.204275\n",
              "37          37     38   355.935376  394.327585\n",
              "38          38     39   356.328853  395.807737\n",
              "39          39     40   354.868159  395.118266\n",
              "40          40     41   355.167474  395.717762\n",
              "41          41     42   353.942184  397.060537\n",
              "42          42     43   354.514065  399.163528\n",
              "43          43     44   354.412752  398.669116\n",
              "44          44     45   352.410634  399.458223\n",
              "45          45     46   353.271284  397.797400\n",
              "46          46     47   353.798015  401.497966\n",
              "47          47     48   352.321050  399.223367\n",
              "48          48     49   352.500059  400.331973\n",
              "49          49     50   350.787606  400.978913\n",
              "50          50     51   351.760088  402.037236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lt9QdsGmyW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "904079a2-37cf-47a9-cc46-1ef65efbbb1b"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "#ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax1.plot(log_output['epoch'],log_output['train_loss'])\n",
        "ax1.plot(log_output['epoch'],log_output['val_loss'])\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9fd003ba90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU0UKxJcvKQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}